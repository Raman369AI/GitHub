{"cells":[{"cell_type":"markdown","id":"7b0627f4-bca1-4b0a-a80e-2eab52d8e380","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n","<h1 align=center><font size = 5>Convolutional Neural Network Simple example </font></h1> \n"]},{"cell_type":"markdown","id":"17141ca6-ffea-4a88-ad9a-8f576135f6f1","metadata":{},"outputs":[],"source":["\n","<h3>Objective for this Notebook<h3>    \n","<h5> 1. Learn Convolutional Neural Network</h5>\n","<h5> 2. Define Softmax, Criterion function, Optimizer and Train the  Model</h5>    \n","\n"]},{"cell_type":"markdown","id":"d0b307cf-41a4-4caa-876f-d07ec0752d0f","metadata":{},"outputs":[],"source":["\n","# Table of Contents\n","In this lab, we will use a Convolutional Neural Networks to classify horizontal an vertical Lines \n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","<li><a href=\"#ref0\">Helper functions </a></li>\n","<li><a href=\"#ref1\"> Prepare Data </a></li>\n","<li><a href=\"#ref2\">Build a Convolutional Neural Network Class </a></li>\n","<li><a href=\"#ref3\">Define the Convolutional Neural Network Classifier, Criterion function, Optimizer and Train the  Model</a></li>\n","<li><a href=\"#ref4\">Analyse Results</a></li>\n","\n","<br>\n","<p></p>\n","Estimated Time Needed: <strong>25 min</strong>\n","</div>\n","\n","<hr>\n"]},{"cell_type":"markdown","id":"15343aa2-93d6-4bd4-be15-2e0c3fc05f9b","metadata":{},"outputs":[],"source":["<a id=\"ref0\"></a>\n","<a name=\"ref0\"><h2 align=center>Helper functions </h2></a>\n"]},{"cell_type":"code","id":"35391291-2e48-46a8-8304-a52f9b5b6e7d","metadata":{},"outputs":[],"source":["import torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd"]},{"cell_type":"code","id":"0d87253a-740b-4d95-82f5-f00aaecf0a87","metadata":{},"outputs":[],"source":["torch.manual_seed(4)"]},{"cell_type":"markdown","id":"c2b9b95f-f6cc-4d30-8ab9-957d16ea8cf6","metadata":{},"outputs":[],"source":["function to plot out the parameters of the Convolutional layers  \n"]},{"cell_type":"code","id":"ae087c2b-03d7-45a1-abed-2ad94744b5b1","metadata":{},"outputs":[],"source":["def plot_channels(W):\n    #number of output channels \n    n_out=W.shape[0]\n    #number of input channels \n    n_in=W.shape[1]\n    w_min=W.min().item()\n    w_max=W.max().item()\n    fig, axes = plt.subplots(n_out,n_in)\n    fig.subplots_adjust(hspace = 0.1)\n    out_index=0\n    in_index=0\n    #plot outputs as rows inputs as columns \n    for ax in axes.flat:\n    \n        if in_index>n_in-1:\n            out_index=out_index+1\n            in_index=0\n              \n        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        in_index=in_index+1\n\n    plt.show()"]},{"cell_type":"markdown","id":"70032ad9-ffb6-4488-a6dd-d0953679a7ce","metadata":{},"outputs":[],"source":["<code>show_data</code>: plot out data sample\n"]},{"cell_type":"code","id":"70295760-43c9-42ae-91c2-c52ca524cefe","metadata":{},"outputs":[],"source":["def show_data(dataset,sample):\n\n    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')\n    plt.title('y='+str(dataset.y[sample].item()))\n    plt.show()"]},{"cell_type":"markdown","id":"0fed8e8c-701e-45dc-87e3-0b6e0accf04c","metadata":{},"outputs":[],"source":["create some toy data \n"]},{"cell_type":"code","id":"22befa81-39ea-47e8-a665-086483936416","metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\nclass Data(Dataset):\n    def __init__(self,N_images=100,offset=0,p=0.9, train=False):\n        \"\"\"\n        p:portability that pixel is wight  \n        N_images:number of images \n        offset:set a random vertical and horizontal offset images by a sample should be less than 3 \n        \"\"\"\n        if train==True:\n            np.random.seed(1)  \n        \n        #make images multiple of 3 \n        N_images=2*(N_images//2)\n        images=np.zeros((N_images,1,11,11))\n        start1=3\n        start2=1\n        self.y=torch.zeros(N_images).type(torch.long)\n\n        for n in range(N_images):\n            if offset>0:\n        \n                low=int(np.random.randint(low=start1, high=start1+offset, size=1))\n                high=int(np.random.randint(low=start2, high=start2+offset, size=1))\n            else:\n                low=4\n                high=1\n        \n            if n<=N_images//2:\n                self.y[n]=0\n                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))\n            elif  n>N_images//2:\n                self.y[n]=1\n                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))\n           \n        \n        \n        self.x=torch.from_numpy(images).type(torch.FloatTensor)\n        self.len=self.x.shape[0]\n        del(images)\n        np.random.seed(0)\n    def __getitem__(self,index):      \n        return self.x[index],self.y[index]\n    def __len__(self):\n        return self.len"]},{"cell_type":"markdown","id":"2c65c3e9-823f-442f-9b6d-984da2d41029","metadata":{},"outputs":[],"source":["<code>plot_activation</code>: plot out the activations of the Convolutional layers  \n"]},{"cell_type":"code","id":"4b228163-b5e7-4a9e-9b0f-805fb75b7841","metadata":{},"outputs":[],"source":["def plot_activations(A,number_rows= 1,name=\"\"):\n    A=A[0,:,:,:].detach().numpy()\n    n_activations=A.shape[0]\n    \n    \n    print(n_activations)\n    A_min=A.min().item()\n    A_max=A.max().item()\n\n    if n_activations==1:\n\n        # Plot the image.\n        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')\n\n    else:\n        fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n        fig.subplots_adjust(hspace = 0.4)\n        for i,ax in enumerate(axes.flat):\n            if i< n_activations:\n                # Set the label for the sub-plot.\n                ax.set_xlabel( \"activation:{0}\".format(i+1))\n\n                # Plot the image.\n                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n                ax.set_xticks([])\n                ax.set_yticks([])\n    plt.show()"]},{"cell_type":"markdown","id":"ea271741-6ea5-4c53-a113-58d88ced540f","metadata":{},"outputs":[],"source":["\n","Utility function for computing output of convolutions\n","takes a tuple of (h,w) and returns a tuple of (h,w)\n"]},{"cell_type":"code","id":"12455756-63d9-4092-9052-80a310757c74","metadata":{},"outputs":[],"source":["\ndef conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n    #by Duane Nielsen\n    from math import floor\n    if type(kernel_size) is not tuple:\n        kernel_size = (kernel_size, kernel_size)\n    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n    return h, w"]},{"cell_type":"markdown","id":"7156bec0-fccd-4619-8d7b-a892994db7ee","metadata":{},"outputs":[],"source":["<a id=\"ref1\"></a>\n","<a name=\"ref1\"><h2 align=center>Prepare Data </h2></a>\n"]},{"cell_type":"markdown","id":"aabdba18-6464-41b0-93ea-1bad1b73d7f6","metadata":{},"outputs":[],"source":["Load the training dataset with 10000 samples \n"]},{"cell_type":"code","id":"f52b42ff-fe89-4d32-8b8e-2ea778e1b90d","metadata":{},"outputs":[],"source":["N_images=10000\ntrain_dataset=Data(N_images=N_images)"]},{"cell_type":"markdown","id":"771a26f3-91c8-4036-b9d6-1a4810ad4248","metadata":{},"outputs":[],"source":["Load the testing dataset\n"]},{"cell_type":"code","id":"8a27ec8b-4489-48d2-9ead-b3991a1112b3","metadata":{},"outputs":[],"source":["validation_dataset=Data(N_images=1000,train=False)\nvalidation_dataset"]},{"cell_type":"markdown","id":"096a2579-c62e-40ef-b101-d2f1a2500445","metadata":{},"outputs":[],"source":["we can see the data type is long \n"]},{"cell_type":"markdown","id":"0a4b7425-64d5-433b-920f-12bf2e63e4b9","metadata":{},"outputs":[],"source":["### Data Visualization \n"]},{"cell_type":"markdown","id":"6680baeb-d67e-435a-a178-e342277e903c","metadata":{},"outputs":[],"source":["Each element in the rectangular  tensor corresponds to a number representing a pixel intensity  as demonstrated by  the following image.\n"]},{"cell_type":"markdown","id":"10c2c199-ec7e-4f6a-82ac-32c55150513e","metadata":{},"outputs":[],"source":["We can print out the third label \n"]},{"cell_type":"code","id":"2a62a229-10c4-4493-96f0-6ad609752366","metadata":{},"outputs":[],"source":["show_data(train_dataset,0)"]},{"cell_type":"code","id":"e5ee8b14-9f3e-434c-9bb6-c5f3722d3750","metadata":{},"outputs":[],"source":["show_data(train_dataset,N_images//2+2)"]},{"cell_type":"markdown","id":"8d797b88-5ae1-458c-830c-a716f7e98d4d","metadata":{},"outputs":[],"source":["we can plot the 3rd  sample \n"]},{"cell_type":"markdown","id":"79705292-2937-4259-8b13-bb7dd0c2dd3c","metadata":{},"outputs":[],"source":["<a id=\"ref2\"></a>\n","<a name=\"ref2\"><h2 align=center>Build a Convolutional Neural Network Class </h2></a> \n","\n"]},{"cell_type":"markdown","id":"5e55ce91-d00f-47db-849f-ae7ab56e929d","metadata":{},"outputs":[],"source":["The input image is 11 x11, the following will change the size of the activations:\n","<ul>\n","<il>convolutional layer</il> \n","</ul>\n","<ul>\n","<il>max pooling layer</il> \n","</ul>\n","<ul>\n","<il>convolutional layer </il>\n","</ul>\n","<ul>\n","<il>max pooling layer </il>\n","</ul>\n","\n","with the following parameters <code>kernel_size</code>, <code>stride</code> and <code> pad</code>.\n","We use the following  lines of code to change the image before we get tot he fully connected layer \n"]},{"cell_type":"code","id":"2956b23b-adf5-49d1-8b85-5835bdc7162d","metadata":{},"outputs":[],"source":["out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out)\nout1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out1)\nout2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out2)\n\nout3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out3)"]},{"cell_type":"markdown","id":"fce4308e-49da-4ce2-85e4-2ba5299f2722","metadata":{},"outputs":[],"source":["Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.\n"]},{"cell_type":"code","id":"fc0fc1f3-2d78-44bf-9b01-21a12004b51c","metadata":{},"outputs":[],"source":["class CNN(nn.Module):\n    def __init__(self,out_1=2,out_2=1):\n        \n        super(CNN,self).__init__()\n        #first Convolutional layers \n        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n\n        #second Convolutional layers\n        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n        #max pooling \n\n        #fully connected layer \n        self.fc1=nn.Linear(out_2*7*7,2)\n        \n    def forward(self,x):\n        #first Convolutional layers\n        x=self.cnn1(x)\n        #activation function \n        x=torch.relu(x)\n        #max pooling \n        x=self.maxpool1(x)\n        #first Convolutional layers\n        x=self.cnn2(x)\n        #activation function\n        x=torch.relu(x)\n        #max pooling\n        x=self.maxpool2(x)\n        #flatten output \n        x=x.view(x.size(0),-1)\n        #fully connected layer\n        x=self.fc1(x)\n        return x\n    \n    def activations(self,x):\n        #outputs activation this is not necessary just for fun \n        z1=self.cnn1(x)\n        a1=torch.relu(z1)\n        out=self.maxpool1(a1)\n        \n        z2=self.cnn2(out)\n        a2=torch.relu(z2)\n        out=self.maxpool2(a2)\n        out=out.view(out.size(0),-1)\n        return z1,a1,z2,a2,out        "]},{"cell_type":"markdown","id":"a3df6d82-c88e-4d9b-9ad5-23e5d1f51883","metadata":{},"outputs":[],"source":["<a id=\"ref3\"></a>\n","<a name=\"ref3\"><h2> Define the Convolutional Neural Network Classifier, Criterion function, Optimizer and Train the  Model</h2></a> \n"]},{"cell_type":"markdown","id":"d23907c1-a223-4626-a12b-f904098fe106","metadata":{},"outputs":[],"source":["There are 2 output channels for the first layer, and 1 outputs channel for the second layer \n"]},{"cell_type":"code","id":"cfe6e89b-c1f2-4cd9-b6c4-a89ba362b7c5","metadata":{},"outputs":[],"source":["model=CNN(2,1)"]},{"cell_type":"markdown","id":"0dbdb38f-97d0-492e-9e18-1e358f850b30","metadata":{},"outputs":[],"source":["we can see the model parameters with the object \n"]},{"cell_type":"code","id":"00c8d5f9-0265-437f-9ff7-59afce8bfaec","metadata":{},"outputs":[],"source":["model"]},{"cell_type":"markdown","id":"b93e58cb-e99e-4856-9f3a-9d80c90291fc","metadata":{},"outputs":[],"source":["Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.\n"]},{"cell_type":"code","id":"591c534e-b918-4aad-b394-144d507af79b","metadata":{},"outputs":[],"source":["\nplot_channels(model.state_dict()['cnn1.weight'])\n"]},{"cell_type":"markdown","id":"25b69e95-c583-4697-ba8b-a865708d45d2","metadata":{},"outputs":[],"source":["Loss function \n"]},{"cell_type":"code","id":"f5b0679d-3a7b-41d0-bca0-580a5b382d2b","metadata":{},"outputs":[],"source":["plot_channels(model.state_dict()['cnn2.weight'])"]},{"cell_type":"markdown","id":"f3abd348-bf16-4a0f-a94d-7a761c025965","metadata":{},"outputs":[],"source":["Define the loss function \n"]},{"cell_type":"code","id":"ed28f181-3aef-49ff-8658-1038975ea139","metadata":{},"outputs":[],"source":["criterion=nn.CrossEntropyLoss()"]},{"cell_type":"markdown","id":"2abe0968-38e8-4e7f-970b-6f8ded1f9933","metadata":{},"outputs":[],"source":[" optimizer class \n"]},{"cell_type":"code","id":"871e0a01-80f3-415b-88ee-1692bb9d01e8","metadata":{},"outputs":[],"source":["learning_rate=0.001\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","id":"8dfd8070-dcd0-42cf-9145-b1d38b346963","metadata":{},"outputs":[],"source":["Define the optimizer class \n"]},{"cell_type":"code","id":"baafa01a-4692-467a-8b90-5002cbeb3370","metadata":{},"outputs":[],"source":["\ntrain_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)\nvalidation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)"]},{"cell_type":"markdown","id":"1af62b0b-d5ae-4c34-8250-f74e3b619374","metadata":{},"outputs":[],"source":["Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**\n"]},{"cell_type":"code","id":"b73a3c0f-8405-4308-ae42-6c9782bce223","metadata":{},"outputs":[],"source":["n_epochs=10\ncost_list=[]\naccuracy_list=[]\nN_test=len(validation_dataset)\ncost=0\n#n_epochs\nfor epoch in range(n_epochs):\n    cost=0    \n    for x, y in train_loader:\n      \n\n        #clear gradient \n        optimizer.zero_grad()\n        #make a prediction \n        z=model(x)\n        # calculate loss \n        loss=criterion(z,y)\n        # calculate gradients of parameters \n        loss.backward()\n        # update parameters \n        optimizer.step()\n        cost+=loss.item()\n    cost_list.append(cost)\n        \n        \n    correct=0\n    #perform a prediction on the validation  data  \n    for x_test, y_test in validation_loader:\n\n        z=model(x_test)\n        _,yhat=torch.max(z.data,1)\n\n        correct+=(yhat==y_test).sum().item()\n        \n\n    accuracy=correct/N_test\n\n    accuracy_list.append(accuracy)\n    \n\n"]},{"cell_type":"markdown","id":"4443cd33-bcde-4247-b20d-ea727e6f5c15","metadata":{},"outputs":[],"source":["#### <a id=\"ref4\"></a>\n","<a name=\"ref4\"><h2 align=center>Analyse Results</h2></a>\n"]},{"cell_type":"markdown","id":"ef0ca6fd-3686-495c-95cc-e112b5dcfdc0","metadata":{},"outputs":[],"source":["Plot the loss and accuracy on the validation data:\n"]},{"cell_type":"code","id":"0102cad7-e4e4-42da-b08c-ad2ff324a704","metadata":{},"outputs":[],"source":["fig, ax1 = plt.subplots()\ncolor = 'tab:red'\nax1.plot(cost_list,color=color)\nax1.set_xlabel('epoch',color=color)\nax1.set_ylabel('total loss',color=color)\nax1.tick_params(axis='y', color=color)\n    \nax2 = ax1.twinx()  \ncolor = 'tab:blue'\nax2.set_ylabel('accuracy', color=color)  \nax2.plot( accuracy_list, color=color)\nax2.tick_params(axis='y', labelcolor=color)\nfig.tight_layout()"]},{"cell_type":"markdown","id":"10a7f6f5-2eeb-4a1e-b038-e6e12450cdb5","metadata":{},"outputs":[],"source":["View the results of the parameters for the Convolutional layers \n"]},{"cell_type":"code","id":"ab6db023-f242-4f48-b866-ce83b40ebf5b","metadata":{},"outputs":[],"source":["model.state_dict()['cnn1.weight']"]},{"cell_type":"code","id":"c46724c5-f142-4281-96a7-957ac13c6a04","metadata":{},"outputs":[],"source":["plot_channels(model.state_dict()['cnn1.weight'])"]},{"cell_type":"code","id":"95391e4d-399b-466d-9944-03498a4d430f","metadata":{},"outputs":[],"source":["model.state_dict()['cnn1.weight']"]},{"cell_type":"code","id":"975ea132-1a35-43ec-8131-77c4c1977ef1","metadata":{},"outputs":[],"source":["plot_channels(model.state_dict()['cnn2.weight'])"]},{"cell_type":"markdown","id":"e9d94b5e-33ac-47ed-a41b-23177979fcfb","metadata":{},"outputs":[],"source":["Consider the following sample \n"]},{"cell_type":"code","id":"3838c6bb-c532-4f74-a1d1-e4c0c4d36d59","metadata":{},"outputs":[],"source":["show_data(train_dataset,N_images//2+2)"]},{"cell_type":"markdown","id":"83e27af8-3367-4535-b53b-c9d74e65b43c","metadata":{},"outputs":[],"source":["Determine the activations \n"]},{"cell_type":"code","id":"1f3f7463-8d4e-4033-a2ea-c3bff7a39092","metadata":{},"outputs":[],"source":["out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))\nout=model.activations(train_dataset[0][0].view(1,1,11,11))"]},{"cell_type":"markdown","id":"85865041-efc8-4c7b-988a-29c8455c36d9","metadata":{},"outputs":[],"source":["Plot them out\n"]},{"cell_type":"code","id":"d7d28b3d-3234-4124-b68c-52a07a27ac85","metadata":{},"outputs":[],"source":["plot_activations(out[0],number_rows=1,name=\" feature map\")\nplt.show()\n"]},{"cell_type":"code","id":"3f3bd943-b1d9-40f9-b66c-8e7593c34f43","metadata":{},"outputs":[],"source":["plot_activations(out[2],number_rows=1,name=\"2nd feature map\")\nplt.show()"]},{"cell_type":"code","id":"b981a7ad-c075-4eb2-8e24-ab9b0d03a1d9","metadata":{},"outputs":[],"source":["plot_activations(out[3],number_rows=1,name=\"first feature map\")\nplt.show()"]},{"cell_type":"markdown","id":"5e759e9d-498b-4de9-84a5-c6dc9375ae7a","metadata":{},"outputs":[],"source":["we save the output of the activation after flattening  \n"]},{"cell_type":"code","id":"b4daa862-1841-424a-9b60-d59507f33974","metadata":{},"outputs":[],"source":["out1=out[4][0].detach().numpy()"]},{"cell_type":"markdown","id":"48fea8d0-84fd-48d7-9a2c-aca979e90d78","metadata":{},"outputs":[],"source":["we can do the same for a sample  where y=0 \n"]},{"cell_type":"code","id":"f3a4f8e5-0390-4012-8b29-23430c8edd58","metadata":{},"outputs":[],"source":["out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()\nout0"]},{"cell_type":"code","id":"613f172d-fe53-4435-806e-7d66f3d8e014","metadata":{},"outputs":[],"source":["plt.subplot(2, 1, 1)\nplt.plot( out1, 'b')\nplt.title('Flatted Activation Values  ')\nplt.ylabel('Activation')\nplt.xlabel('index')\nplt.subplot(2, 1, 2)\nplt.plot(out0, 'r')\nplt.xlabel('index')\nplt.ylabel('Activation')"]},{"cell_type":"markdown","id":"53a0897b-6c81-4c3d-8919-89004fe0de1a","metadata":{},"outputs":[],"source":["\n","\n","<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"></a>\n","\n"]},{"cell_type":"markdown","id":"8e90f9dc-0b8d-4798-a47c-c54bc26d0889","metadata":{},"outputs":[],"source":["### About the Authors:  \n","[Joseph Santarcangelo]( https://www.linkedin.com/in/joseph-s-50398b136/) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. \n","\n","Other contributors: [Michelle Carey](  https://www.linkedin.com/in/michelleccarey/) \n"]},{"cell_type":"markdown","id":"c61cd49a-05f9-472c-91e4-073eff1924ef","metadata":{},"outputs":[],"source":["<!--\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n","\n","\n","<hr>\n","-->\n","\n","## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"prev_pub_hash":"a52c626bae0836e780d9fc3789d4a7038a36aa68d3201241c323a06b7dd54d25"},"nbformat":4,"nbformat_minor":4}