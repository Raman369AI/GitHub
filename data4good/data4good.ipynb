{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('examples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = {\n",
    "    0: \"Attended a previous Regional or Nationals seminar\",\n",
    "    1: \"To connect with other survivors who share a similar loss\",\n",
    "    2: \"To learn more about resources TAPS has to offer\",\n",
    "    3: \"To learn new tools and information to help me with my grief\",\n",
    "    4: \"To learn more about how to support my adult family members in their grief\",\n",
    "    5: \"To learn more about how to support my child(ren) in their grief\",\n",
    "    6: \"For my child(ren) to attend Good Grief Camp\",\n",
    "    7: \"For my child(ren) to connect with a Military Mentor\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['q1'] = data['q1'].map(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = {\n",
    "    0: \"I found this event while searching for grief resources\",\n",
    "    1: \"I attended a seminar last year and had already marked my calendar!\",\n",
    "    2: \"TAPS invited me to this event via email\",\n",
    "    3: \"My TAPS Survivor Care Team Member invited me\",\n",
    "    4: \"My Peer Mentor or another survivor invited me\",\n",
    "    5: \"Through the TAPS website\",\n",
    "    6: \"Through the TAPS magazine\",\n",
    "    7: \"Through a TAPS Social Media Page\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['q2'] = data['q2'].map(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             Through the TAPS magazine\n",
       "1         My Peer Mentor or another survivor invited me\n",
       "2                              Through the TAPS website\n",
       "3                                                   NaN\n",
       "4     I found this event while searching for grief r...\n",
       "5     I attended a seminar last year and had already...\n",
       "6         My Peer Mentor or another survivor invited me\n",
       "7                      Through a TAPS Social Media Page\n",
       "8                             Through the TAPS magazine\n",
       "9                      Through a TAPS Social Media Page\n",
       "10         My TAPS Survivor Care Team Member invited me\n",
       "11                                                  NaN\n",
       "12                             Through the TAPS website\n",
       "13                             Through the TAPS website\n",
       "14                                                  NaN\n",
       "15                                                  NaN\n",
       "16                                                  NaN\n",
       "17              TAPS invited me to this event via email\n",
       "18                                                  NaN\n",
       "19                     Through a TAPS Social Media Page\n",
       "20                     Through a TAPS Social Media Page\n",
       "21    I found this event while searching for grief r...\n",
       "22    I found this event while searching for grief r...\n",
       "23                                                  NaN\n",
       "24              TAPS invited me to this event via email\n",
       "25    I found this event while searching for grief r...\n",
       "26                            Through the TAPS magazine\n",
       "27        My Peer Mentor or another survivor invited me\n",
       "28                                                  NaN\n",
       "29                                                  NaN\n",
       "30                             Through the TAPS website\n",
       "31                                                  NaN\n",
       "32                                                  NaN\n",
       "33                     Through a TAPS Social Media Page\n",
       "34                             Through the TAPS website\n",
       "35                                                  NaN\n",
       "36                                                  NaN\n",
       "37    I attended a seminar last year and had already...\n",
       "38    I attended a seminar last year and had already...\n",
       "39              TAPS invited me to this event via email\n",
       "40                                                  NaN\n",
       "41    I found this event while searching for grief r...\n",
       "42                                                  NaN\n",
       "43                             Through the TAPS website\n",
       "44                     Through a TAPS Social Media Page\n",
       "45        My Peer Mentor or another survivor invited me\n",
       "46    I attended a seminar last year and had already...\n",
       "47              TAPS invited me to this event via email\n",
       "48                     Through a TAPS Social Media Page\n",
       "49                                                  NaN\n",
       "Name: q2, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['q2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = {\n",
    "    0: \"I Disagree that I am socially connected\", \n",
    "    1: \"I Neither agree nor disagree that I am socially connected\",\n",
    "    2: \"Agree that I am socially connected\"\n",
    "}\n",
    "\n",
    "q4 = {\n",
    "    0: \"I Disagree that I cope up with grief\",\n",
    "    1: \"Neither agree nor disagree that I cope up with grief\",\n",
    "    2: \"Agree that I cope up with grief\"\n",
    "}\n",
    "\n",
    "q5 = {\n",
    "    0: \"Disagree that I better understand my grief\",\n",
    "    1: \"Neither agree nor disagree that I better understand my grief\",\n",
    "    2: \"Agree that I better understand my grief\"\n",
    "}\n",
    "\n",
    "q6 = {\n",
    "    0: \"Disagree that I have hope for future\",\n",
    "    1: \"Neither agree nor disagree that I have hope for future\",\n",
    "    2: \"Agree that I have hope for future\"\n",
    "}\n",
    "\n",
    "q7 = {\n",
    "    0: \"Extremely likely / Very likely to continue therapy\",\n",
    "    1: \"Somewhat likely to continue therapy\",\n",
    "    2: \"Neither likely nor unlikely / Neutral to continue therapy\",\n",
    "    3: \"Somewhat unlikely to continue therapy\",\n",
    "    4: \"Extremely unlikely / Not at all likely to continue therapy\"\n",
    "}\n",
    "\n",
    "q8 = {\n",
    "    0: \"Extremely likely / Very likely\",\n",
    "    1: \"Somewhat likely\",\n",
    "    2: \"Neither likely nor unlikely / Neutral\",\n",
    "    3: \"Somewhat unlikely\",\n",
    "    4: \"Extremely unlikely / Not at all likely\"\n",
    "}\n",
    "\n",
    "q9 = {\n",
    "    0: \"Strongly Disagree that I have positive outlook of life\",\n",
    "    1: \"Disagree that I have positive outlook of life\",\n",
    "    2: \"Agree that I have positive outlook of life\",\n",
    "    3: \"Strongly Agree that I have positive outlook of life\"\n",
    "}\n",
    "\n",
    "q10 = {\n",
    "    0: \"Strongly Disagree I have short and/or long range goals\",\n",
    "    1: \"Disagree I have short and/or long range goals\",\n",
    "    2: \"Agree I have short and/or long range goals\",\n",
    "    3: \"Strongly Agree I have short and/or long range goals\"\n",
    "}\n",
    "\n",
    "q11 = {\n",
    "    0: \"Strongly Disagree I feel all alone\",\n",
    "    1: \"Disagree I feel all alone\",\n",
    "    2: \"Agree I feel all alone\",\n",
    "    3: \"Strongly Agree I feel all alone\"\n",
    "}\n",
    "\n",
    "q12 = {\n",
    "    0: \"Strongly Disagree I can see possibilities in the midst of difficulties\",\n",
    "    1: \"Disagree I can see possibilities in the midst of difficulties\",\n",
    "    2: \"Agree I can see possibilities in the midst of difficulties\",\n",
    "    3: \"Strongly Agree I can see possibilities in the midst of difficulties\"\n",
    "}\n",
    "\n",
    "q13 = {\n",
    "    0: \"Strongly Disagree I have faith that gives me comfort\",\n",
    "    1: \"Disagree I have faith that gives me comfort\",\n",
    "    2: \"Agree I have faith that gives me comfort\",\n",
    "    3: \"Strongly Agree I have faith that gives me comfort\"\n",
    "}\n",
    "\n",
    "q14 = {\n",
    "    0: \"Strongly Disagree I feel scared about my future\",\n",
    "    1: \"Disagree I feel scared about my future\",\n",
    "    2: \"Agree I feel scared about my future\",\n",
    "    3: \"Strongly Agree I feel scared about my future\"\n",
    "}\n",
    "\n",
    "q15 = {\n",
    "    0: \"Strongly Disagree I can recall happy/joyful times\",\n",
    "    1: \"Disagree I can recall happy/joyful times\",\n",
    "    2: \"Agree I can recall happy/joyful times\",\n",
    "    3: \"Strongly Agree I can recall happy/joyful times\"\n",
    "}\n",
    "\n",
    "q16 = {\n",
    "    0: \"Strongly Disagree I have deep inner strength\",\n",
    "    1: \"Disagree I have deep inner strength\",\n",
    "    2: \"Agree I have deep inner strength\",\n",
    "    3: \"Strongly Agree I have deep inner strength\"\n",
    "}\n",
    "\n",
    "q17 = {\n",
    "    0: \"Strongly Disagree I am able to give and receive care/love\",\n",
    "    1: \"Disagree I am able to give and receive care/love\",\n",
    "    2: \"Agree I am able to give and receive care/love\",\n",
    "    3: \"Strongly Agree I am able to give and receive care/love\"\n",
    "}\n",
    "\n",
    "q18 = {\n",
    "    0: \"Strongly Disagree  I have a sense of direction\",\n",
    "    1: \"Disagree  I have a sense of direction\",\n",
    "    2: \"Agree  I have a sense of direction\",\n",
    "    3: \"Strongly Agree  I have a sense of direction\"\n",
    "}\n",
    "\n",
    "q19 = {\n",
    "    0: \"Strongly Disagree I believe that each day has potential\",\n",
    "    1: \"Disagree I believe that each day has potential\",\n",
    "    2: \"Agree I believe that each day has potential\",\n",
    "    3: \"Strongly Agree I believe that each day has potential\"\n",
    "}\n",
    "\n",
    "q20 = {\n",
    "    0: \"Strongly Disagree I feel my life has value and worth\",\n",
    "    1: \"Disagree I feel my life has value and worth\",\n",
    "    2: \"Agree I feel my life has value and worth\",\n",
    "    3: \"Strongly Agree I feel my life has value and worth\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['q3'] = data['q3'].map(q3)\n",
    "data['q4'] = data['q4'].map(q4)\n",
    "data['q5'] = data['q5'].map(q5)\n",
    "data['q6'] = data['q6'].map(q6)\n",
    "data['q7'] = data['q7'].map(q7)\n",
    "data['q8'] = data['q8'].map(q8)\n",
    "data['q9'] = data['q9'].map(q9)\n",
    "data['q10'] = data['q10'].map(q10)\n",
    "data['q11'] = data['q11'].map(q11)\n",
    "data['q12'] = data['q12'].map(q12)\n",
    "data['q13'] = data['q13'].map(q13)\n",
    "data['q14'] = data['q14'].map(q14)\n",
    "data['q15'] = data['q15'].map(q15)\n",
    "data['q16'] = data['q16'].map(q16)\n",
    "data['q17'] = data['q17'].map(q17)\n",
    "data['q18'] = data['q18'].map(q18)\n",
    "data['q19'] = data['q19'].map(q19)\n",
    "data['q20'] = data['q20'].map(q20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['q21'] = data['q21'].fillna('Did not answer')\n",
    "data['q22'] = data['q22'].fillna('Did not answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['q1'] = test['q1'].map(q1)\n",
    "test['q2'] = test['q2'].map(q2)\n",
    "test['q3'] = test['q3'].map(q3)\n",
    "test['q4'] = test['q4'].map(q4)\n",
    "test['q5'] = test['q5'].map(q5)\n",
    "test['q6'] = test['q6'].map(q6)\n",
    "test['q7'] = test['q7'].map(q7)\n",
    "test['q8'] = test['q8'].map(q8)\n",
    "test['q9'] = test['q9'].map(q9)\n",
    "test['q10'] = test['q10'].map(q10)\n",
    "test['q11'] = test['q11'].map(q11)\n",
    "test['q12'] = test['q12'].map(q12)\n",
    "test['q13'] = test['q13'].map(q13)\n",
    "test['q14'] = test['q14'].map(q14)\n",
    "test['q15'] = test['q15'].map(q15)\n",
    "test['q16'] = test['q16'].map(q16)\n",
    "test['q17'] = test['q17'].map(q17)\n",
    "test['q18'] = test['q18'].map(q18)\n",
    "test['q19'] = test['q19'].map(q19)\n",
    "test['q20'] = test['q20'].map(q20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['q2'] = test['q2'].fillna(-1)\n",
    "data['q2'] = data['q2'].fillna(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['q21'] = test['q21'].fillna('Did not answer')\n",
    "test['q22'] = test['q22'].fillna('Did not answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.columns:\n",
    "    if data[i].dtype == 'int64':\n",
    "        data[i] = data[i].astype('str')\n",
    "    elif data[i].dtype == 'float64':\n",
    "        data[i] = data[i].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test.columns:\n",
    "    if test[i].dtype == 'int64':\n",
    "        test[i] = test[i].astype('str')\n",
    "    elif test[i].dtype == 'float64':\n",
    "        test[i] = test[i].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.columns:\n",
    "    data[i] = data[i].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test.columns:\n",
    "    test[i] = test[i].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['combined'] = data[['q1', 'q2', 'q3', 'q4', 'q5', 'q6', 'q7', 'q8', 'q9', 'q10', 'q11', 'q12', 'q13', 'q14', 'q15', 'q16', 'q17', 'q18', 'q19', 'q20', 'q21', 'q22']].agg(','.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['combined'] = test[['q1', 'q2', 'q3', 'q4', 'q5', 'q6', 'q7', 'q8', 'q9', 'q10', 'q11', 'q12', 'q13', 'q14', 'q15', 'q16', 'q17', 'q18', 'q19', 'q20', 'q21', 'q22']].agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['combined'] = data['combined'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['combined'] = test['combined'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = data['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = data['label'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kronos/anaconda3/envs/rapids-24.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Make sure to install and download spaCy's English model:\n",
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatize_sentence_spacy(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['combined'] = test['combined'].apply(lambda x: lemmatize_sentence_spacy(x))\n",
    "data['combined'] = data['combined'].apply(lambda x: lemmatize_sentence_spacy(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kronos/anaconda3/envs/rapids-24.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "data_ds = Dataset.from_pandas(data, split = \"train\")\n",
    "test_ds = Dataset.from_pandas(test, split = \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Id', 'q1', 'q2', 'q3', 'q4', 'q5', 'q6', 'q7', 'q8', 'q9', 'q10', 'q11', 'q12', 'q13', 'q14', 'q15', 'q16', 'q17', 'q18', 'q19', 'q20', 'q21', 'q22', 'label', 'combined'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ds = data_ds.remove_columns(['Id', 'q1', 'q2', 'q3', 'q4', 'q5', 'q6', 'q7', 'q8', 'q9', 'q10', 'q11', 'q12', 'q13', 'q14', 'q15', 'q16', 'q17', 'q18', 'q19', 'q20', 'q21', 'q22'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_ds.remove_columns(['Id', 'q1', 'q2', 'q3', 'q4', 'q5', 'q6', 'q7', 'q8', 'q9', 'q10', 'q11', 'q12', 'q13', 'q14', 'q15', 'q16', 'q17', 'q18', 'q19', 'q20', 'q21', 'q22'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 3]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ds['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 02:42:46.245640: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-12 02:42:46.258972: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739349766.274290   29946 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739349766.278809   29946 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-12 02:42:46.296058: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "num_labels = 6\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", num_labels = num_labels, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def compute_metrics(pred):\n",
    " labels = pred.label_ids\n",
    " preds = pred.predictions.argmax(-1)\n",
    " f1 = f1_score(labels, preds, average=\"weighted\")\n",
    " acc = accuracy_score(labels, preds)\n",
    " return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(output_dir='./',\n",
    " num_train_epochs=500,\n",
    " learning_rate=2e-5,\n",
    " per_device_train_batch_size=16,\n",
    " weight_decay=0.02,\n",
    " disable_tqdm=False,\n",
    " log_level=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    # Tokenize the text column (e.g., \"combined\")\n",
    "    tokenized_inputs = tokenizer(batch[\"combined\"], padding=True, truncation=True)\n",
    "    # Re-add the labels to the tokenized output\n",
    "    tokenized_inputs[\"label\"] = batch[\"label\"]\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 849.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_ds_encoded = data_ds.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_test(batch):\n",
    "    # Tokenize the text column (e.g., \"combined\")\n",
    "    tokenized_inputs = tokenizer(batch[\"combined\"], padding=True, truncation=True)\n",
    "    # Re-add the labels to the tokenized output\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2950 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2950/2950 [00:02<00:00, 1083.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_ds_encoded = test_ds.map(tokenize_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['combined', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2950\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29946/110968179.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model, args=training_args,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 22:20, Epoch 500/500]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=2.5569639285095037e-05, metrics={'train_runtime': 1340.8579, 'train_samples_per_second': 18.645, 'train_steps_per_second': 1.492, 'total_flos': 3261089422583136.0, 'train_loss': 2.5569639285095037e-05, 'epoch': 500.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(model=model, args=training_args,\n",
    " compute_metrics=compute_metrics,\n",
    " train_dataset=data_ds_encoded,\n",
    " tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_output = trainer.predict(test_ds_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[10.753874 , -3.3913286, -2.7251413, -4.773441 , -4.502895 ,\n",
       "        -3.7150362],\n",
       "       [-1.0087496, -0.9509897,  0.826017 , -3.1217983,  0.6154793,\n",
       "        -6.14898  ],\n",
       "       [ 1.34921  ,  1.5526679,  3.858885 , -5.9745927, -4.7261987,\n",
       "        -6.2717266],\n",
       "       ...,\n",
       "       [ 4.080142 , -2.630808 , -4.271771 , -6.0838723, -4.1096373,\n",
       "         3.6629539],\n",
       "       [-2.8194017, -3.3210838, -4.3836327, -4.5493693, -2.3742778,\n",
       "        10.124219 ],\n",
       "       [11.011597 , -3.2177947, -3.600213 , -4.32363  , -4.3499336,\n",
       "        -3.7354965]], dtype=float32), label_ids=None, metrics={'test_runtime': 48.4554, 'test_samples_per_second': 60.881, 'test_steps_per_second': 7.615})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.argmax(preds_output.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, ..., 0, 5, 0])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = y_preds + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'] = y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       3\n",
       "2       3\n",
       "3       1\n",
       "4       5\n",
       "       ..\n",
       "2945    4\n",
       "2946    3\n",
       "2947    1\n",
       "2948    6\n",
       "2949    1\n",
       "Name: label, Length: 2950, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Id = test['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame({ 'Id': Id, 'label': y_preds })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
