# -*- coding: utf-8 -*-
"""Amazon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x-N7n9mkG1vAcul5gztNKMjZEaES-TRl
"""

import pandas as pd
import re
import nltk

reviews = pd.read_csv('/content/all_reviews.csv')

nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('vader_lexicon')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

reviews

reviews['first_letter'] = reviews['rating'].str[0]

reviews['first_letter'] = reviews['first_letter'].astype('int')

reviews['body'] = reviews['body'].apply(lambda x: re.sub(r"\s+", " ", x))

reviews['body'] = reviews['body'].apply(lambda x: re.sub(r"[^A-Za-z0-9 ]", " ", x))

reviews['body'] = reviews['body'].apply(lambda x: x.lower())

import nltk
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from nltk.corpus import wordnet

# Download necessary NLTK resources if not already downloaded
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

# Helper function to convert NLTK's POS tags to WordNet's format
def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # Default to noun if no match

lemmatizer = WordNetLemmatizer()

# Apply lemmatization with POS tagging
reviews['body'] = reviews['body'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tag(x.split())]))

comment = nltk.TweetTokenizer()

reviews['tokens'] = reviews['body'].apply(lambda x: comment.tokenize(x))

tokens = reviews['tokens']

reviews

tokens = tokens.to_list()

stopwords = nltk.corpus.stopwords.words('english')
features = []
for i in tokens:
  for j in i:
    if j not in stopwords:
      features.append(j)
print(features)

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(features)
feature_names = tfidf_vectorizer.get_feature_names_out()
avg_tfidf_scores = np.mean(tfidf_matrix.toarray(), axis=0)
features_scores = zip(feature_names, avg_tfidf_scores)
sorted_features = sorted(features_scores, key=lambda x: x[1], reverse=True)
top_10_features = sorted_features[:15]
print("Top 10 features by average TF-IDF score:")
for feature, score in top_10_features:
    print(f"{feature}: {score}")

top_10 = []
for i in top_10_features:
   top_10.append(i[0])
top_10

import pandas as pd
from nltk.probability import FreqDist
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Assuming 'all_aspects' contains all the aspects extracted from the reviews
# You can use the same 'all_aspects' list from the previous step where aspects were aggregated

# Frequency distribution of aspects
aspect_freq = FreqDist(top_10)

# Generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color ='white').generate_from_frequencies(aspect_freq)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")  # No axis details
plt.show()

p = reviews['tokens'].apply(nltk.pos_tag)

p

pos = []
for i in p:
  for j in i:
    if j[0] in top_10:
     pos.append(j)
pd.Series(pos).nunique()

p = [i for j in p for i in j]

from nltk.chunk import *
from nltk.chunk.util import *
from nltk.chunk.regexp import *
from nltk import Tree

chunk_rule = """
  NP: {<DT>?<JJ>*<NN.*>+}
  VP: {<VB.*>+}
  ADJP: {<JJ>+}
"""

word_count = 0
chunk_parser = RegexpParser(chunk_rule)
chunked_tree = chunk_parser.parse(p)
priority_components = ['NP', 'VP', 'JJ']
summary_parts = []
for subtree in chunked_tree.subtrees():
  if subtree.label() in priority_components and word_count < 20:
        phrase = " ".join(word for word, tag in subtree.leaves())
        summary_parts.append(phrase)
        word_count += len(phrase.split())
if word_count >= 20:
  summary = " ".join(summary_parts)
print(summary)

from nltk.sentiment import SentimentIntensityAnalyzer

sia = SentimentIntensityAnalyzer()

reviews['sentiment'] = reviews['tokens'].apply(lambda x: sia.polarity_scores(' '.join(x)))

reviews['compound'] = reviews['sentiment'].apply(lambda x: x['compound'])

reviews['compound'][reviews['compound']>0.5].count()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
bins = [-1, -0.5, 0.5, 1]
sns.histplot(data=reviews['compound'], bins=bins)
plt.xlabel('Value Ranges')
plt.ylabel('Count')
plt.title('Distribution of Values')
plt.show()

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import pandas as pd
from transformers import pipeline

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

# Load your data
# reviews = pd.read_csv('path_to_reviews.csv')

# Initialize stopwords for English
stop_words = set(stopwords.words('english'))

def extract_nouns(text):
    words = word_tokenize(text)
    words_pos = pos_tag(words)
    nouns = [word for word, pos in words_pos if pos.startswith('NN') and word.lower() not in stop_words]
    return ' '.join(nouns)

# Extract nouns from each review
reviews['nouns'] = reviews['body'].apply(extract_nouns)

# Initialize and fit CountVectorizer
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(reviews['nouns'])

# Initialize and fit LDA
lda = LatentDirichletAllocation(n_components=10, random_state=0)
lda.fit(X)

# Get feature names
feature_names = vectorizer.get_feature_names_out()

# Gather top 50 words for each topic
topics_features = []
for topic_idx, topic in enumerate(lda.components_):
    top_features_indices = topic.argsort()[-50:]  # Select top 50 features
    top_features = [feature_names[i] for i in top_features_indices]
    topics_features.append(" ".join(top_features))

# Join all topic-representative words into a single text
combined_topic_text = " ".join(topics_features)

combined_topic_text

import pandas as pd
from nltk.probability import FreqDist
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Assuming 'all_aspects' contains all the aspects extracted from the reviews
# You can use the same 'all_aspects' list from the previous step where aspects were aggregated

# Frequency distribution of aspects
aspect_freq = FreqDist(feature_names)

# Generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color ='white').generate_from_frequencies(aspect_freq)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")  # No axis details
plt.show()

# Initialize the summarization pipeline using BERT
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Function to prepare text for summarization (include full text + emphasized topics)
def prepare_summary_text(full_text, topics_text):
    return f"{topics_text} {full_text}"  # Prepend topics to the full text to emphasize them in the summary

# Create combined text for summarization
reviews['combined_for_summary'] = reviews.apply(lambda row: prepare_summary_text(row['body'], combined_topic_text), axis=1)

# Summarize the combined text
try:
    summary_result = summarizer(reviews['combined_for_summary'].iloc[0], max_length=80, min_length=10, do_sample=False)  # Example for the first review
    full_summary = summary_result[0]['summary_text']
except Exception as e:
    print(f"Error during summarization: {e}")
    full_summary = ""

# Print the combined summary
print(full_summary)

import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D
from tensorflow.keras.optimizers import Adam

# Hyperparameters
vocab_size = len(top_10)
embedding_dim = 100  # Vocabulary size
max_length = 200
padding_type = 'post'     # Maximum length of each sentence
trunc_type = 'post' # Truncate from the end of the sequence if it exceeds max_length
oov_tok = '<OOV>'   # Out of vocabulary token

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(reviews["body"])
sequences = tokenizer.texts_to_sequences(reviews["body"])
padded_sequences = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)

labels = np.array(reviews["first_letter"])

labels = pd.Series(labels)
labels = labels.apply(lambda x: 1 if x >= 3 else 0)
labels = labels.to_numpy()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense, Flatten
from tensorflow.keras.regularizers import l1

# Correcting the max_length to match the padded sequences
max_length = 100
padded_sequences = pad_sequences(sequences, maxlen=100, truncating=trunc_type)
padded_sequences = np.expand_dims(padded_sequences, -1)
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.3, random_state=42)
model = Sequential([
    Embedding(input_dim=1000, output_dim=32, input_length=max_length),  # Smaller embedding
    SpatialDropout1D(0.1),
    Bidirectional(LSTM(64, dropout=0.1, recurrent_dropout=0.1)),  # Smaller LSTM, more dropout
    Flatten(),
    Dense(64, activation='ReLU', kernel_regularizer=l1(0.01)),  # Regularization
    Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer=Adam(0.0001), metrics=['accuracy'])

print(model.summary())

history = model.fit(X_train, y_train, epochs=80,batch_size=32, validation_data=(X_test, y_test), verbose=1)

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dropout, Dense, Flatten, TimeDistributed, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l1
from sklearn.model_selection import train_test_split

# Adjust max_length and padding to match the new requirements
max_length = 100
padded_sequences = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)
padded_sequences = np.expand_dims(padded_sequences, -1)  # Ensure padded_sequences has the right shape

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.3, random_state=42)

# Model architecture
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=32, input_length=max_length),  # Assume vocab_size is defined
    SpatialDropout1D(0.1),
    Bidirectional(LSTM(64, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)),  # Enable return_sequences
    LSTM(32, return_sequences=False, dropout=0.1, recurrent_dropout=0.1),  # Additional LSTM layer
    Dense(64, activation='relu', kernel_regularizer=l1(0.01)),  # Dense layer for regularizing
    Dense(vocab_size, activation='softmax')  # Final layer with softmax for word prediction
])

# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(0.0001), metrics=['accuracy'])

# Print model summary
print(model.summary())

from transformers import BertModel, BertConfig, BertTokenizer

configuration = BertConfig()
model = BertModel(configuration)
configuration = model.config

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = TFBertModel.from_pretrained('bert-base-uncased')
texts = reviews['body'].to_list()
# Tokenize the texts and prepare input tensors
encodings = tokenizer(texts, max_length=512, truncation=True, padding=True, return_tensors='tf')

import pandas as pd
import nltk
from textblob import TextBlob
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.probability import FreqDist

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

# Load the data
file_path = '/content/all_reviews.csv'
data = pd.read_csv(file_path)

# Function to extract nouns as aspects
def extract_aspects(text):
    stop_words = set(stopwords.words('english'))
    words = word_tokenize(text)
    words = [word for word in words if word.isalpha() and word not in stop_words]
    tags = nltk.pos_tag(words)
    nouns = [word for word, pos in tags if pos.startswith('NN')]
    return nouns



# Extract aspects from all comments
all_aspects = data['body'].apply(extract_aspects).sum()  # This flattens the list of lists into a single list

# Frequency distribution of aspects
aspect_freq = FreqDist(all_aspects)
top_aspects = aspect_freq.most_common(10)

# Calculate sentiment for top aspects

# Display the top aspects and their sentiments
print("Top Aspects and Their Sentiments:")
print(top_aspect_sentiments)

top_aspect_sentiments

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(aspects)
feature_names = tfidf_vectorizer.get_feature_names_out()
avg_tfidf_scores = np.mean(tfidf_matrix.toarray(), axis=0)
features_scores = zip(feature_names, avg_tfidf_scores)
sorted_features = sorted(features_scores, key=lambda x: x[1], reverse=True)
top_10_features = sorted_features[:15]
print("Top 10 features by average TF-IDF score:")
for feature, score in top_10_features:
    print(f"{feature}: {score}")

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

# Load the data
file_path = '/content/all_reviews.csv'
data = pd.read_csv(file_path)

# Function to extract nouns as aspects
def extract_aspects(text):
    stop_words = set(stopwords.words('english'))
    words = word_tokenize(text)
    words = [word for word in words if word.isalpha() and word not in stop_words]
    tags = nltk.pos_tag(words)
    nouns = [word for word, pos in tags if pos.startswith('NN')]
    return nouns

# Extract aspects from all comments
all_aspects = data['body'].apply(extract_aspects).sum()  # This flattens the list of lists into a single list

# Frequency distribution of aspects
aspect_freq = FreqDist(all_aspects)
top_aspects = aspect_freq.most_common(10)

# Display the top aspects
print("Top 10 Aspects:")
for aspect, frequency in top_aspects:
    print(f"{aspect}: {frequency}")

import pandas as pd
import nltk
from textblob import TextBlob
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.tag import pos_tag
from nltk.probability import FreqDist

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

# Load the data
file_path = '/content/all_reviews.csv'
data = pd.read_csv(file_path)

# Function to extract nouns as aspects
def extract_aspects(text):
    stop_words = set(stopwords.words('english'))
    words = word_tokenize(text)
    words = [word for word in words if word.isalpha() and word not in stop_words]
    tags = pos_tag(words)
    nouns = [word for word, pos in tags if pos.startswith('NN')]
    return nouns

# Extract aspects from all comments and identify the top aspects
all_aspects = data['body'].apply(extract_aspects).sum()
aspect_freq = FreqDist(all_aspects)
top_aspects = [aspect for aspect, freq in aspect_freq.most_common(15)]

# Function to detect aspects and their context in text
def aspect_sentiment(text):
    tokens = word_tokenize(text)
    tagged = pos_tag(tokens)
    aspect_details = {}

    for i, (word, tag) in enumerate(tagged):
        if word in top_aspects:
            # Context window (previous and next two words)
            context_words = tokens[max(i-3, 0):i] + tokens[i+1:i+4]
            context_tags = pos_tag(context_words)
            # Extract adjectives and verbs only
            filtered_context = " ".join([w for w, t in context_tags if t.startswith(('JJ', 'VB'))])
            full_context = " ".join([filtered_context, word])  # Adding the aspect to the context
            sentiment = TextBlob(full_context).sentiment.polarity
            aspect_details[word] = {'context': full_context, 'sentiment': sentiment}
    return aspect_details

# Apply the aspect detection and sentiment analysis
data['aspect_details'] = data['body'].apply(aspect_sentiment)

# Display the results
print(data[['body', 'aspect_details']].head())

import pandas as pd
import numpy as np

# Assuming 'data' is your existing DataFrame and it already contains the 'aspect_details' field

# Function to transform aspect details into a structured format
def transform_aspect_data(row):
    aspects = {}
    for aspect, details in row['aspect_details'].items():
        aspects[f"{aspect}"] = details['sentiment']
    return aspects

# Apply the transformation to each row and create a new DataFrame
aspect_data = data.apply(transform_aspect_data, axis=1, result_type='expand')

# Combine the original body text with the new aspect sentiment columns
final_data = pd.concat([data['body'], aspect_data], axis=1)

# Display the final DataFrame
print(final_data.head())

import pandas as pd

# Initialize total counters
total_positive = 0
total_neutral = 0
total_negative = 0

# Number of columns excluding the 'body' (first column)
num_columns = len(final_data.columns) - 1

# Loop through all columns starting from the second column
for column in final_data.columns[1:]:  # This skips the 'body' column
    # Count the number of cells with positive sentiment scores in the current column
    positive_sentiment_count = (final_data[column] > 0).mean()

    # Count the number of cells with neutral sentiment scores in the current column (assuming neutral is exactly 0)
    neutral_sentiment_count = (final_data[column] == 0).mean()

    # Count the number of cells with negative sentiment scores in the current column
    negative_sentiment_count = (final_data[column] < 0).mean()

    # Update totals


    # Display the counts for the current column
    print(f"Column: {column}")
    print(f"AverAGE of cells with positive sentiment: {positive_sentiment_count}")
    print(f"Number of cells with neutral sentiment: {neutral_sentiment_count}")
    print(f"Number of cells with negative sentiment: {negative_sentiment_count}\n")

# Display total counts, adjusting the message to reflect the count per column average if necessary

import pandas as pd

# Initialize counters for each sentiment type
total_positive = 0
total_neutral = 0
total_negative = 0

# Loop through each row in the DataFrame
for index, row in final_data.iterrows():
    # Check if any column in the row has positive, neutral, or negative sentiment
    has_positive = any(row[col] > 0 for col in final_data.columns[1:])
    has_neutral = any(row[col] == 0 for col in final_data.columns[1:]) and not has_positive and not has_negative
    has_negative = any(row[col] < 0 for col in final_data.columns[1:]) and not has_positive and not has_negative

    # Update the total counts based on the checks above
    if has_positive:
        total_positive += 1
    if has_neutral:
        total_neutral += 1
    if has_negative:
        total_negative += 1

# Print the total counts
print(f"Total positive sentiment across comments: {total_positive}")
print(f"Total neutral sentiment across comments: {total_neutral}")
print(f"Total negative sentiment across comments: {total_negative}")



final_data.to_csv('final.csv')

import pandas as pd
from nltk.probability import FreqDist
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Assuming 'all_aspects' contains all the aspects extracted from the reviews
# You can use the same 'all_aspects' list from the previous step where aspects were aggregated

# Frequency distribution of aspects
aspect_freq = FreqDist(all_aspects)

# Generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color ='white').generate_from_frequencies(aspect_freq)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")  # No axis details
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Define your string variables
BERT = "Protein powder delivers a punch of protein to my drink but it also leave me feel satisfied and energize for hour the taste be incredible with a rich and creamy flavor that make each sip a delight what set this protein powder apart be it filling nature it keep hunger at bay make it an ideal choice for post workout recovery or a a meal replacement."
GPT = "Customers appreciate the flavor, texture, and mixability of the protein powders. Many reviews highlight the variety and pleasantness of flavors, noting how the powders are easy to mix and have a satisfying texture. There's general satisfaction with the products' effectiveness and value for money. However, opinions vary on certain aspects such as specific tastes and aftertastes, with some concerns about grittiness in a few products"
pos = "i ve try numerous protein powder the year none compare the deliciousness satiety this protein powder do deliver a punch​"

text_to_compare = "This is a text for comparison."

# Create a TF-IDF vectorizer to represent the text as numerical vectors
vectorizer = TfidfVectorizer()

# Fit the vectorizer and transform the texts
texts = [BERT, GPT, pos]
vectors = vectorizer.fit_transform(texts + [text_to_compare])

cosine_similarities = cosine_similarity(vectors)

print("Cosine similarity between Amazon and BART:", cosine_similarities[3, 0])
print("Cosine similarity between Amazon and GPT:", cosine_similarities[3, 1])
print("Cosine similarity between Amazon and POS:", cosine_similarities[3, 2])
print("Cosine similarity between Amazon and POS:", cosine_similarities[3, 3])

nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('vader_lexicon')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')