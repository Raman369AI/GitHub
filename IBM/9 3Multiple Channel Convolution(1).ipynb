{"cells":[{"cell_type":"markdown","id":"a5e6ca06-aca6-422f-ba57-9a70ef8b296b","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n","\n","\n","\n","<h1>Multiple Input and Output Channels</h1> \n"]},{"cell_type":"markdown","id":"bb4169f3-6c7f-4c34-881a-c4c1645cda23","metadata":{},"outputs":[],"source":["\n","<h3>Objective for this Notebook<h3>    \n","<h5> 1. Learn on Multiple Input and Multiple Output Channels.</h5>    \n","\n"]},{"cell_type":"markdown","id":"5e2083a7-5b08-44db-b83f-83210e5d08c7","metadata":{},"outputs":[],"source":["\n","# Table of Contents\n","In this lab, you will study convolution and review how the different operations change the relationship between input and output.\n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","<li><a href=\"#ref0\">Multiple Output Channels </a></li>\n","\n","<li><a href=\"#ref1\">Multiple Input Channels</a></li>\n","<li><a href=\"#ref2\">Multiple Input and Multiple Output Channels </a></li>\n","<li><a href=\"#ref3\">Practice Questions </a></li>\n","\n","<br>\n","<p></p>\n","Estimated Time Needed: <strong>25 min</strong>\n","</div>\n","\n","<hr>\n"]},{"cell_type":"markdown","id":"ede01ba2-f291-4f40-9bc3-43ca548319f0","metadata":{},"outputs":[],"source":["Import the following libraries:\n"]},{"cell_type":"code","id":"08373742-1ab5-4dbd-bd5d-9bcc3db01428","metadata":{},"outputs":[],"source":["import torch \nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import ndimage, misc"]},{"cell_type":"markdown","id":"7e263114-8276-4d8c-bf18-53b196fbdd98","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"eabc5580-355b-4804-bbdd-d35bc44cf09d","metadata":{},"outputs":[],"source":["<a id=\"ref0\"></a>\n","<h2 align=center>Multiple Output Channels </h2>\n"]},{"cell_type":"markdown","id":"d49e4430-4c04-4035-92e5-d55e16a950db","metadata":{},"outputs":[],"source":["In Pytroch, you can create a <code>Conv2d</code> object with multiple outputs. For each channel, a kernel is created, and each kernel performs a convolution independently. As a result, the number of outputs is equal to the number of channels. This is demonstrated in the following figure. The number 9 is convolved with three kernels: each of a different color. There are three different activation maps represented by the different colors.\n"]},{"cell_type":"markdown","id":"9da8d8b4-d0b9-448e-a583-06318ddb6225","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2activationmaps.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"5fc3094c-62df-45cf-91ac-e913a83da90e","metadata":{},"outputs":[],"source":["Symbolically, this can be represented as follows:\n"]},{"cell_type":"markdown","id":"a2471518-78c2-40b8-accb-18970cbbe438","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2activationmap2.png\" width=\"500,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"65d06814-a638-4219-b5c8-29ddf4017c22","metadata":{},"outputs":[],"source":["Create a <code>Conv2d</code> with three channels:\n"]},{"cell_type":"code","id":"c56d7b76-4ac0-4679-9687-73ba786e77ed","metadata":{},"outputs":[],"source":["conv1 = nn.Conv2d(in_channels=1, out_channels=3,kernel_size=3)"]},{"cell_type":"markdown","id":"8fbb3f8a-fe39-45e9-a184-87c6dde77d2a","metadata":{},"outputs":[],"source":["Pytorch randomly assigns values to each kernel. However, use kernels that have  been developed to detect edges:\n"]},{"cell_type":"code","id":"53720f2d-9653-40b6-b972-0d2b0071e77c","metadata":{},"outputs":[],"source":["Gx=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\nGy=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])\n\nconv1.state_dict()['weight'][0][0]=Gx\nconv1.state_dict()['weight'][1][0]=Gy\nconv1.state_dict()['weight'][2][0]=torch.ones(3,3)"]},{"cell_type":"markdown","id":"b6a1905f-76c8-4c88-a8cc-aff31c7617da","metadata":{},"outputs":[],"source":["Each kernel has its own bias, so set them all to zero:\n"]},{"cell_type":"code","id":"8cb7753d-8ba0-4cf5-834c-7184a808bb16","metadata":{},"outputs":[],"source":["conv1.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0])\nconv1.state_dict()['bias']"]},{"cell_type":"markdown","id":"f34ecb94-7fe6-4cbe-bf5a-0e9b23ceaf3e","metadata":{},"outputs":[],"source":["Print out each kernel: \n"]},{"cell_type":"code","id":"f6442135-363e-4903-a932-47d3c4112892","metadata":{},"outputs":[],"source":["for x in conv1.state_dict()['weight']:\n    print(x)"]},{"cell_type":"markdown","id":"18094ab3-7138-46c3-8c9c-91d306a16eb1","metadata":{},"outputs":[],"source":["Create an input <code>image</code> to represent the input X:\n"]},{"cell_type":"code","id":"03993021-4c05-4cd6-9a3d-04eb317b8f20","metadata":{},"outputs":[],"source":["image=torch.zeros(1,1,5,5)\nimage[0,0,:,2]=1\nimage"]},{"cell_type":"markdown","id":"191057bd-76bd-48aa-8460-23aab381b6e9","metadata":{},"outputs":[],"source":["Plot it as an image: \n"]},{"cell_type":"code","id":"4a47aff5-9725-4b04-8110-a0395b76f5b2","metadata":{},"outputs":[],"source":["plt.imshow(image[0,0,:,:].numpy(), interpolation='nearest', cmap=plt.cm.gray)\nplt.colorbar()\nplt.show()"]},{"cell_type":"markdown","id":"6f3b2077-8d5e-4e04-b571-aadfc1667696","metadata":{},"outputs":[],"source":["Perform convolution using each channel: \n"]},{"cell_type":"code","id":"e157d1f7-6ba0-4989-aa36-8ec102a292b0","metadata":{},"outputs":[],"source":["out=conv1(image)"]},{"cell_type":"markdown","id":"d9f946ef-58c5-4b13-b871-dfd12663c68f","metadata":{},"outputs":[],"source":["The result is a 1x3x3x3 tensor. This represents one sample with three channels, and each channel contains a 3x3 image.  The same rules that govern the shape of each image were discussed in the last section.\n"]},{"cell_type":"code","id":"5000c916-2dd8-48d6-bb17-91ce5e0aa90a","metadata":{},"outputs":[],"source":["out.shape"]},{"cell_type":"markdown","id":"692ec842-9903-4565-8ad3-83f98aa282df","metadata":{},"outputs":[],"source":["Print out each channel as a tensor or an image: \n"]},{"cell_type":"code","id":"210db566-25c8-40bc-ac5e-a5dcb39c0c73","metadata":{},"outputs":[],"source":["for channel,image in enumerate(out[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()"]},{"cell_type":"markdown","id":"280fcf9e-7eb7-4307-9cdc-720dda18d7fc","metadata":{},"outputs":[],"source":["Different kernels can be used to detect various features in an image. You can see that the first channel fluctuates, and the second two channels produce a constant value. The following figure summarizes the process:\n"]},{"cell_type":"markdown","id":"34fdb998-a477-4c00-b9b8-9595a504b8ab","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2outputsgray.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"d6e53caa-6aa1-4914-aa4c-7530fecf0e81","metadata":{},"outputs":[],"source":["If you use a different image, the result will be different: \n"]},{"cell_type":"code","id":"bf1b8afd-121a-40cc-821d-77f2ad48e0a7","metadata":{},"outputs":[],"source":["image1=torch.zeros(1,1,5,5)\nimage1[0,0,2,:]=1\nprint(image1)\nplt.imshow(image1[0,0,:,:].detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\nplt.show()"]},{"cell_type":"markdown","id":"09410848-f417-48c8-84a2-087c41e5f4dd","metadata":{},"outputs":[],"source":["In this case, the second channel fluctuates, and the first and the third channels produce a constant value.\n"]},{"cell_type":"code","id":"06581bbf-a128-4afa-878e-748aed91e9ee","metadata":{},"outputs":[],"source":["out1=conv1(image1)\nfor channel,image in enumerate(out1[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()"]},{"cell_type":"markdown","id":"1d16d421-968d-4c73-b91c-a132e3e0dcef","metadata":{},"outputs":[],"source":["The following figure summarizes the process:\n"]},{"cell_type":"markdown","id":"43d16b56-8b2e-40d2-9b4f-359594fc75ff","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2ouputsgray2.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"38acf41b-fecf-414d-b0b4-d435511d20df","metadata":{},"outputs":[],"source":["<a id=\"ref1\"></a>\n","<h2 align=center>Multiple Input Channels </h2>\n"]},{"cell_type":"markdown","id":"6441ad37-de67-47b0-ac2f-5ec121d5ae09","metadata":{},"outputs":[],"source":["For two inputs, you can create two kernels. Each kernel performs a convolution on its associated input channel. The resulting output is added together as shown:  \n"]},{"cell_type":"markdown","id":"871a0b53-14d9-4ddc-bddb-3205ab6a7b3d","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.22chanalsinput.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"96371046-ccac-4112-b608-e5e827b513f5","metadata":{},"outputs":[],"source":["Create an input with two channels:\n"]},{"cell_type":"code","id":"47e5dd30-0d09-4ba1-814d-c676b2a96d22","metadata":{},"outputs":[],"source":["image2=torch.zeros(1,2,5,5)\nimage2[0,0,2,:]=-2\nimage2[0,1,2,:]=1\nimage2"]},{"cell_type":"markdown","id":"3e9fcbb1-8edc-4652-b6ce-faeac5d25bc4","metadata":{},"outputs":[],"source":["Plot out each image: \n"]},{"cell_type":"code","id":"f4179699-9dd3-432b-87e0-e80f4fae9dda","metadata":{},"outputs":[],"source":["for channel,image in enumerate(image2[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()"]},{"cell_type":"markdown","id":"8680a826-5a29-428a-9210-878f2bf15d1c","metadata":{},"outputs":[],"source":["Create a <code>Conv2d</code> object with two inputs:\n"]},{"cell_type":"code","id":"91b5e536-3628-4b78-b358-243b6213ccea","metadata":{},"outputs":[],"source":["conv3 = nn.Conv2d(in_channels=2, out_channels=1,kernel_size=3)"]},{"cell_type":"markdown","id":"1cb98575-643f-48d2-8084-0ba40e063c17","metadata":{},"outputs":[],"source":["Assign kernel values to make the math a little easier: \n"]},{"cell_type":"code","id":"b27d2d26-f455-4405-83f4-f7bd65a4e1d1","metadata":{},"outputs":[],"source":["Gx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]])\nconv3.state_dict()['weight'][0][0]=1*Gx1\nconv3.state_dict()['weight'][0][1]=-2*Gx1\nconv3.state_dict()['bias'][:]=torch.tensor([0.0])"]},{"cell_type":"code","id":"91433f26-be6f-4a55-8ca7-19eaeee0dd3a","metadata":{},"outputs":[],"source":["conv3.state_dict()['weight']"]},{"cell_type":"markdown","id":"6ff7fbbe-3f8b-4e55-be4a-1ff71e575691","metadata":{},"outputs":[],"source":["Perform the convolution:\n"]},{"cell_type":"code","id":"79890efc-3616-4561-92f4-4fea94d3d790","metadata":{},"outputs":[],"source":["conv3(image2)"]},{"cell_type":"markdown","id":"aded5bf8-aec7-4a19-b50c-8dbdd5e93ba8","metadata":{},"outputs":[],"source":["The following images summarize the process. The object performs Convolution.\n"]},{"cell_type":"markdown","id":"e4e9b0f2-ec35-4c93-9a59-36306e1c5f1c","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_two_channal_example.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"379062df-123f-419c-9f51-d4448a64234e","metadata":{},"outputs":[],"source":["Then, it adds the result: \n"]},{"cell_type":"markdown","id":"92f5c80a-38a0-4fc4-b957-5f3f29cb055a","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_two_channal_example2.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"1e7bf5a5-0cb3-45f1-8687-24908ff3e0f6","metadata":{},"outputs":[],"source":["<a id=\"ref2\"></a>\n","\n","<h2>Multiple Input and Multiple Output Channels</h2>\n"]},{"cell_type":"markdown","id":"f4d9a138-2747-4e9a-9666-b70a02504f15","metadata":{},"outputs":[],"source":["When using multiple inputs and outputs, a kernel is created for each input, and the process is repeated for each output. The process is summarized in the following image. \n","\n","There are two input channels and 3 output channels. For each channel, the input in red and purple is convolved with an individual kernel that is colored differently. As a result, there are three outputs. \n"]},{"cell_type":"markdown","id":"dedbf98b-294c-4845-ae07-eb4c09903400","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2mulit_input_output.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"0e7e3d1a-bdc9-45a6-8b0b-2f775a6edc36","metadata":{},"outputs":[],"source":["Create an example with two inputs and three outputs and assign the kernel values to make the math a little easier: \n"]},{"cell_type":"code","id":"02a8ec34-2996-45e5-9c0c-7330a684ff36","metadata":{},"outputs":[],"source":["conv4 = nn.Conv2d(in_channels=2, out_channels=3,kernel_size=3)\nconv4.state_dict()['weight'][0][0]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])\nconv4.state_dict()['weight'][0][1]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])\n\n\nconv4.state_dict()['weight'][1][0]=torch.tensor([[0.0,0.0,0.0],[0,1,0],[0.0,0.0,0.0]])\nconv4.state_dict()['weight'][1][1]=torch.tensor([[0.0,0.0,0.0],[0,-1,0],[0.0,0.0,0.0]])\n\nconv4.state_dict()['weight'][2][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\nconv4.state_dict()['weight'][2][1]=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])"]},{"cell_type":"markdown","id":"63f4f161-cbe3-4037-bd7e-b85843f00119","metadata":{},"outputs":[],"source":["For each output, there is a bias, so set them all to zero: \n"]},{"cell_type":"code","id":"13c159fd-2df8-48e9-994c-90b154470601","metadata":{},"outputs":[],"source":["conv4.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0])"]},{"cell_type":"markdown","id":"a8fc9ad6-7784-4595-ad8e-e02383c90212","metadata":{},"outputs":[],"source":["Create a two-channel image and plot the results: \n"]},{"cell_type":"code","id":"1ca1082a-7ef6-400d-8cda-a10c77992721","metadata":{},"outputs":[],"source":["image4=torch.zeros(1,2,5,5)\nimage4[0][0]=torch.ones(5,5)\nimage4[0][1][2][2]=1\nfor channel,image in enumerate(image4[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()"]},{"cell_type":"markdown","id":"5161600d-2fbd-496a-a55e-1546fafb1087","metadata":{},"outputs":[],"source":["Perform the convolution:\n"]},{"cell_type":"code","id":"2703fe0a-c97b-482f-8918-03da1841ea09","metadata":{},"outputs":[],"source":["z=conv4(image4)\nz"]},{"cell_type":"markdown","id":"34f94605-c515-48be-a9ac-88901befcd0f","metadata":{},"outputs":[],"source":["The output of the first channel is given by: \n"]},{"cell_type":"markdown","id":"85c3c43d-2aa2-4468-8925-0454a6c8f68f","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_%20multi_channel_1.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"a18d7ef1-e8dc-4738-a7b9-30d0a66db993","metadata":{},"outputs":[],"source":["The output of the second channel is given by:\n"]},{"cell_type":"markdown","id":"ddc48d86-5187-4ea7-ae0e-070bf85936e1","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_%20multi_channel_2.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"417421ea-f889-4c6c-869b-e3f8c251f511","metadata":{},"outputs":[],"source":["The output of the third channel is given by: \n"]},{"cell_type":"markdown","id":"c915c385-8562-4f40-aba3-c34c89c0008a","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_%20multi_channel_3.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"538755d7-09a6-4e4a-8435-4ec31e0e6602","metadata":{},"outputs":[],"source":["<a id=\"ref3\"></a>\n","\n","<h2>Practice Questions </h2>\n"]},{"cell_type":"markdown","id":"1867105d-563a-4dcb-9cf7-6250b91afd17","metadata":{},"outputs":[],"source":["Use the following two convolution objects to produce the same result as two input channel convolution on imageA and imageB as shown in the following image:\n"]},{"cell_type":"code","id":"ad69b530-9d72-4906-b743-97e77fafd3a1","metadata":{},"outputs":[],"source":["imageA=torch.zeros(1,1,5,5)\nimageB=torch.zeros(1,1,5,5)\nimageA[0,0,2,:]=-2\nimageB[0,0,2,:]=1\n\n\nconv5 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\nconv6 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\n\n\nGx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]])\nconv5.state_dict()['weight'][0][0]=1*Gx1\nconv6.state_dict()['weight'][0][0]=-2*Gx1\nconv5.state_dict()['bias'][:]=torch.tensor([0.0])\nconv6.state_dict()['bias'][:]=torch.tensor([0.0])"]},{"cell_type":"markdown","id":"a3bfebce-c14c-4475-898e-efc2fafd2d73","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2Practice%20Questions_1.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"081f2cd8-b8f2-4a0c-b193-0f3663262284","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2Practice%20Questions_2.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","id":"996d3b9e-45a4-40c7-8965-bd00152153dd","metadata":{},"outputs":[],"source":["Double-click __here__ for the solution.\n","\n","<!-- Your answer is below:\n","conv5(imageA)+conv6(imageB)\n","-->\n","\n","\n"]},{"cell_type":"markdown","id":"adce8af1-341d-4b47-bc8a-e7013d77cee4","metadata":{},"outputs":[],"source":["\n","<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"></a>\n"]},{"cell_type":"markdown","id":"c2b04ff2-6899-40e0-a8de-4ecef231089e","metadata":{},"outputs":[],"source":["### About the Authors:  \n","[Joseph Santarcangelo]( https://www.linkedin.com/in/joseph-s-50398b136/) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. \n","\n","Other contributors: [Michelle Carey](  https://www.linkedin.com/in/michelleccarey/), [Mavis Zhou](  https://www.linkedin.com/in/jiahui-mavis-zhou-a4537814a/) \n"]},{"cell_type":"markdown","id":"b292d99f-a94e-4bc0-a948-70a4cb714c59","metadata":{},"outputs":[],"source":["<!--\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n","\n","\n","<hr>\n","-->\n","\n","## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"d23796fe0a44c6681b5d85a378a3d35942062b77174dfa20ef98898dd0321cb3"},"nbformat":4,"nbformat_minor":4}