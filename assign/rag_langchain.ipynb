{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader  # EPUB files can be converted to PDFs if needed\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-aYENQwZ-p_c"
   },
   "outputs": [],
   "source": [
    "# If running in Google Colab, you may need to run this cell to make sure you're using UTF-8 locale to install LangChain\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-26 21:36:32--  https://www.gutenberg.org/ebooks/1342.epub.noimages\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 2610:28:3090:3000:0:bad:cafe:47, 152.19.134.47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|2610:28:3090:3000:0:bad:cafe:47|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.gutenberg.org/cache/epub/1342/pg1342.epub [following]\n",
      "--2024-11-26 21:36:33--  https://www.gutenberg.org/cache/epub/1342/pg1342.epub\n",
      "Reusing existing connection to [www.gutenberg.org]:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 561345 (548K) [application/epub+zip]\n",
      "Saving to: ‘./test/library/jane-austen/pride-and-prejudice.epub’\n",
      "\n",
      "./test/library/jane 100%[===================>] 548.19K   533KB/s    in 1.0s    \n",
      "\n",
      "2024-11-26 21:36:34 (533 KB/s) - ‘./test/library/jane-austen/pride-and-prejudice.epub’ saved [561345/561345]\n",
      "\n",
      "--2024-11-26 21:36:34--  https://www.gutenberg.org/ebooks/135.epub.noimages\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 2610:28:3090:3000:0:bad:cafe:47, 152.19.134.47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|2610:28:3090:3000:0:bad:cafe:47|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.gutenberg.org/cache/epub/135/pg135.epub [following]\n",
      "--2024-11-26 21:36:35--  https://www.gutenberg.org/cache/epub/135/pg135.epub\n",
      "Reusing existing connection to [www.gutenberg.org]:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1846329 (1.8M) [application/epub+zip]\n",
      "Saving to: ‘./test/library/victor-hugo/les-miserables.epub’\n",
      "\n",
      "./test/library/vict 100%[===================>]   1.76M  4.78MB/s    in 0.4s    \n",
      "\n",
      "2024-11-26 21:36:35 (4.78 MB/s) - ‘./test/library/victor-hugo/les-miserables.epub’ saved [1846329/1846329]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p \"./test/library/jane-austen\"\n",
    "!mkdir -p \"./test/library/victor-hugo\"\n",
    "!wget https://www.gutenberg.org/ebooks/1342.epub.noimages -O \"./test/library/jane-austen/pride-and-prejudice.epub\"\n",
    "!wget https://www.gutenberg.org/ebooks/135.epub.noimages -O \"./test/library/victor-hugo/les-miserables.epub\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kronos/anaconda3/envs/pytorch_env/lib/python3.12/site-packages/ebooklib/epub.py:1423: FutureWarning: This search incorrectly ignores the root element, and will be fixed in a future version.  If you rely on the current behaviour, change it to './/xmlns:rootfile[@media-type]'\n",
      "  for root_file in tree.findall('//xmlns:rootfile[@media-type]', namespaces={'xmlns': NAMESPACES['CONTAINERNS']}):\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def extract_text(file_path):\n",
    "    \"\"\"Extract plain text content from an EPUB file.\"\"\"\n",
    "    book = epub.read_epub(file_path)\n",
    "    extracted_text = []\n",
    "    for item in book.get_items():\n",
    "        if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "            soup = BeautifulSoup(item.content, 'html.parser')\n",
    "            extracted_text.append(soup.get_text())\n",
    "    return \"\\n\".join(extracted_text)\n",
    "\n",
    "# Function to split text into chunks\n",
    "def split_text_into_chunks(text, size=512, overlap=30):\n",
    "    \"\"\"Split the input text into manageable chunks with optional overlap.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=size, chunk_overlap=overlap)\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# Extract text from EPUB files\n",
    "pride_and_prejudice_text = extract_text(\"./test/library/jane-austen/pride-and-prejudice.epub\")\n",
    "les_miserables_text = extract_text(\"./test/library/victor-hugo/les-miserables.epub\")\n",
    "\n",
    "# Chunk the extracted text\n",
    "pride_and_prejudice_chunks = split_text_into_chunks(pride_and_prejudice_text)\n",
    "les_miserables_chunks = split_text_into_chunks(les_miserables_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_docs = pride_and_prejudice_chunks + les_miserables_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ixmCdRzBQ5gu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32133/1448490164.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5')\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "documents = [Document(page_content=chunk) for chunk in chunked_docs]\n",
    "\n",
    "# Initialize the HuggingFace Embedding Model (Dense Embedding)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5')\n",
    "\n",
    "# Create the FAISS vector store from the documents\n",
    "db = FAISS.from_documents(documents, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kronos/anaconda3/envs/pytorch_env/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Make sure the token is passed correctly\n",
    "reranker = pipeline(\"text-classification\", model=\"cross-encoder/nli-deberta-v3-base\", truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import AnalyzeDocumentChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Initialize the CrossEncoder for reranking\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-TinyBERT-L-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the symptoms of diabetes?\"\n",
    "retrieved_docs = db.similarity_search(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='sugar. O nibbling sex, your pretty little white teeth adore sugar. Now,\\nheed me well, sugar is a salt. All salts are withering. Sugar is the most\\ndesiccating of all salts; it sucks the liquids of the blood through the\\nveins; hence the coagulation, and then the solidification of the blood;\\nhence tubercles in the lungs, hence death. That is why diabetes borders on\\nconsumption. Then, do not crunch sugar, and you will live. I turn to the'),\n",
       " Document(metadata={}, page_content='felt. Is any one the less ill because one does not know the name of one’s\\nmalady?')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_results = []\n",
    "for doc in retrieved_docs:\n",
    "    # Combine the query and document for relevance scoring\n",
    "    result = reranker.predict([(query, doc.page_content)])\n",
    "    score = result[0]  # This should directly give you the score\n",
    "    reranked_results.append((score, doc.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0011, Document: sugar. O nibbling sex, your pretty little white teeth adore sugar. Now,\n",
      "heed me well, sugar is a salt. All salts are withering. Sugar is the most\n",
      "desiccating of all salts; it sucks the liquids of the blood through the\n",
      "veins; hence the coagulation, and then the solidification of the blood;\n",
      "hence tubercles in the lungs, hence death. That is why diabetes borders on\n",
      "consumption. Then, do not crunch sugar, and you will live. I turn to the\n",
      "Score: 0.0009, Document: felt. Is any one the less ill because one does not know the name of one’s\n",
      "malady?\n"
     ]
    }
   ],
   "source": [
    "reranked_results.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Display re-ranked results\n",
    "for score, doc in reranked_results:\n",
    "    print(f\"Score: {score:.4f}, Document: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "L-ggaa763VRo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:18<00:00,  2.28s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the history of the diabetes?\"\n",
    "context = \" \".join([doc for _, doc in reranked_results])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVNRJALyXYHG"
   },
   "source": [
    "## Setup the LLM chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUUNneJ1smhl"
   },
   "source": [
    "Finally, we have all the pieces we need to set up the LLM chain.\n",
    "\n",
    "First, create a text_generation pipeline using the loaded model and its tokenizer.\n",
    "\n",
    "Next, create a prompt template - this should follow the format of the model, so if you substitute the model checkpoint, make sure to use the appropriate formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "cR0k1cRWz8Pm"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=400,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "prompt_template = f\"\"\"\n",
    "You are an AI assistant trained to answer questions based solely on provided context. \n",
    "Do not invent any information. Only use the following documents to answer the query.\n",
    "Question: {query}\n",
    "Context: {context}\n",
    "\n",
    "Please provide a concise and accurate answer without hallucinations:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "llm_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l19UKq5HXfSp"
   },
   "source": [
    "Note: _You can also use `tokenizer.apply_chat_template` to convert a list of messages (as dicts: `{'role': 'user', 'content': '(...)'}`) into a string with the appropriate chat format._\n",
    "\n",
    "\n",
    "Finally, we need to combine the `llm_chain` with the retriever to create a RAG chain. We pass the original question through to the final generation step, as well as the retrieved context docs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_rI3YNp9Xl4s"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "rag_chain = (\n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsCOhfDDXpaS"
   },
   "source": [
    "## Compare the results\n",
    "\n",
    "Let's see the difference RAG makes in generating answers to the library-specific questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "W7F07fQLXusU"
   },
   "outputs": [],
   "source": [
    "question = \"How do you check diabetes?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KC0rJYU1x1ir"
   },
   "source": [
    "First, let's see what kind of answer we can get with just the model itself, no context added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "GYh-HG1l0De5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are an AI assistant trained to answer questions based solely on provided context. \\nDo not invent any information. Only use the following documents to answer the query.\\nQuestion: What is the history of the diabetes?\\nContext: sugar. O nibbling sex, your pretty little white teeth adore sugar. Now,\\nheed me well, sugar is a salt. All salts are withering. Sugar is the most\\ndesiccating of all salts; it sucks the liquids of the blood through the\\nveins; hence the coagulation, and then the solidification of the blood;\\nhence tubercles in the lungs, hence death. That is why diabetes borders on\\nconsumption. Then, do not crunch sugar, and you will live. I turn to the felt. Is any one the less ill because one does not know the name of one’s\\nmalady?\\n\\nPlease provide a concise and accurate answer without hallucinations:\\n\\nDiabetes is a condition characterized by high levels of sugar (glucose) in the blood due to either insufficient production or resistance to the effects of insulin, a hormone that regulates blood sugar levels. This can lead to complications such as nerve damage, kidney disease, and increased risk of heart disease and stroke. Diabetes has been linked to other health conditions, including consumptive diseases like tuberculosis, which may explain the statement \"diabetes borders on consumption.\" However, it\\'s essential to note that while diabetes can increase the risk of certain illnesses, it doesn\\'t necessarily mean that someone with diabetes will develop them. It\\'s crucial to manage diabetes through lifestyle changes and medication to prevent complications.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\"context\":\"\", \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-TIWr3wx9w8"
   },
   "source": [
    "As you can see, the model interpreted the question as one about physical computer adapters, while in the context of PEFT, \"adapters\" refer to LoRA adapters.\n",
    "Let's see if adding context from GitHub issues helps the model give a more relevant answer:"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
