{"cells":[{"cell_type":"markdown","id":"3485b1eb-48ab-4da5-963f-643bc7c823b3","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"f59b472a-96fc-4ff8-beb2-89dae723a7e7","metadata":{},"outputs":[],"source":["# **Lab: Building Autoencoders**\n"]},{"cell_type":"markdown","id":"7dcfb575-d416-4483-ad4c-108e2b6c61fc","metadata":{},"outputs":[],"source":["Estimated time needed: **30** minutes\n"]},{"cell_type":"markdown","id":"2d6b3cf0-18f9-46a8-83cb-9f59a357436d","metadata":{},"outputs":[],"source":["In this lab, you will learn how to build autoencoders using Keras.  \n"]},{"cell_type":"markdown","id":"5fe109c6-f7ba-4449-a534-6628f9ff494b","metadata":{},"outputs":[],"source":["## Learning Objectives\n","\n","By the end of this lab, you will: \n","\n","- Load and preprocess the MNIST dataset for training an autoencoder. \n","\n","- Construct a simple autoencoder model using the Keras functional API. \n","\n","- Train the autoencoder on the MNIST dataset. \n","\n","- Evaluate the performance of the trained autoencoder. \n","\n","- Fine-tune the autoencoder to improve its performance. \n","\n","- Use the autoencoder to denoise images. \n"]},{"cell_type":"markdown","id":"574eb04a-66ba-4dd0-bd06-4c33067da796","metadata":{},"outputs":[],"source":["----\n"]},{"cell_type":"markdown","id":"33be3c22-74ca-47ac-a4b3-a4d750948669","metadata":{},"outputs":[],"source":["### Step-by-Step Instructions: \n","\n","#### Step 1: Data Preprocessing \n","\n","This exercise prepares the MNIST dataset for training by normalizing the pixel values and flattening the images. Normalization helps in faster convergence during training, and flattening is required because the input layer of our autoencoder expects a one-dimensional vector. \n"]},{"cell_type":"code","id":"d3a0f593-0a0c-4aa3-b75c-59ca90eb1754","metadata":{},"outputs":[],"source":["!pip install tensorflow==2.16.2"]},{"cell_type":"code","id":"fcbd1e02-da37-4fbd-9610-c79576bea589","metadata":{},"outputs":[],"source":["import numpy as np \nfrom tensorflow.keras.datasets import mnist \n\n# Load the dataset \n(x_train, _), (x_test, _) = mnist.load_data() \n\n# Normalize the pixel values \nx_train = x_train.astype('float32') / 255. \nx_test = x_test.astype('float32') / 255. \n\n# Flatten the images \nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:]))) \nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:]))) "]},{"cell_type":"markdown","id":"efe9efec-4d8d-45d1-b978-908b70d90aee","metadata":{},"outputs":[],"source":["In the above code: \n","- Use Keras to load the MNIST dataset. \n","- Normalize the image pixel values to the range [0, 1]. \n","- Flatten the 28x28 images to a 784-dimensional vector to reshape the data. \n"]},{"cell_type":"markdown","id":"38d71aa0-5aee-4aa7-bcf7-80d998c90781","metadata":{},"outputs":[],"source":["#### Step 2: Building the Autoencoder Model \n","\n","This exercise involves building an autoencoder with an encoder that compresses the input to 32 dimensions and a decoder that reconstructs the input from these 32 dimensions. The model is compiled with the Adam optimizer and binary crossentropy loss. \n"]},{"cell_type":"code","id":"209ec1f6-2e7d-481c-b5af-20f9fbf202f9","metadata":{},"outputs":[],"source":["from tensorflow.keras.models import Model \nfrom tensorflow.keras.layers import Input, Dense \n\n# Encoder \ninput_layer = Input(shape=(784,)) \nencoded = Dense(64, activation='relu')(input_layer) \n\n# Bottleneck \nbottleneck = Dense(32, activation='relu')(encoded) \n\n# Decoder \ndecoded = Dense(64, activation='relu')(bottleneck) \noutput_layer = Dense(784, activation='sigmoid')(decoded) \n\n# Autoencoder model \nautoencoder = Model(input_layer, output_layer) \n\n# Compile the model \nautoencoder.compile(optimizer='adam', loss='binary_crossentropy') \n\n# Summary of the model \nautoencoder.summary() "]},{"cell_type":"markdown","id":"62da7ac3-03d5-40cd-b138-8fc47e9c4738","metadata":{},"outputs":[],"source":["In the above code: \n","\n","**1. Define the Encoder:**\n","- Create an input layer with 784 neurons. \n","- Add a Dense layer with 64 neurons and ReLU activation. \n","\n","**2. Define the Bottleneck:**\n","- Add a Dense layer with 32 neurons and ReLU activation. \n","\n","**3. Define the Decoder:**\n","- Add a Dense layer with 64 neurons and ReLU activation. \n","- Add an output layer with 784 neurons and sigmoid activation. \n","\n","**4. Compile the Model:**\n","- Use the Adam optimizer and binary crossentropy loss.  \n"]},{"cell_type":"markdown","id":"a3ceda7a-9f5c-4908-b6b2-5ad6a27e1675","metadata":{},"outputs":[],"source":["#### Step 3: Training the Autoencoder \n","\n","In this exercise, the autoencoder is trained to reconstruct the MNIST images. The training data is both the input and the target, as the autoencoder learns to map the input to itself. \n"]},{"cell_type":"code","id":"c3198d10-1fc1-4c8e-bffd-25ecab7fc801","metadata":{},"outputs":[],"source":["autoencoder.fit(\n    x_train, x_train,  \n    epochs=25,  \n    batch_size=256,  \n    shuffle=True,  \n    validation_data=(x_test, x_test)\n)"]},{"cell_type":"markdown","id":"aa5ef3ad-2eb3-453e-8a2d-adf487219000","metadata":{},"outputs":[],"source":["In the above code: \n","- Use the `fit` method to train the model on the training data. \n","- Set the number of epochs to 25 and the batch size to 256.. \n","- Use the test data for validation. \n"]},{"cell_type":"markdown","id":"f80ae479-149a-4bf1-bedf-2a0704132f5e","metadata":{},"outputs":[],"source":["#### Step 4: Evaluating the Autoencoder \n","\n","This exercise evaluates the autoencoder by reconstructing the test images and comparing them to the original images. Visualization helps in understanding how well the autoencoder has learned to reconstruct the input data. \n"]},{"cell_type":"code","id":"058bbf5a-05dd-4ccc-9344-c8ad8352be9c","metadata":{},"outputs":[],"source":["!pip install matplotlib==3.9.2"]},{"cell_type":"code","id":"82606a5b-792f-44c1-895c-ac8bf0168dbf","metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt \n\n# Predict the test data \nreconstructed = autoencoder.predict(x_test) \n\n# Visualize the results \nn = 10  # Number of digits to display \nplt.figure(figsize=(20, 4)) \n\nfor i in range(n): \n    # Display original \n    ax = plt.subplot(2, n, i + 1) \n    plt.imshow(x_test[i].reshape(28, 28)) \n    plt.gray() \n    ax.get_xaxis().set_visible(False) \n    ax.get_yaxis().set_visible(False) \n\n    # Display reconstruction \n    ax = plt.subplot(2, n, i + 1 + n) \n    plt.imshow(reconstructed[i].reshape(28, 28)) \n    plt.gray() \n    ax.get_xaxis().set_visible(False) \n    ax.get_yaxis().set_visible(False) \n\nplt.show()\n"]},{"cell_type":"markdown","id":"d2a61df8-cbf4-4813-b638-269db0a053a1","metadata":{},"outputs":[],"source":["In the above code: \n","\n","**1. Reconstruct Images:**\n","- Use the autoencoder to predict the test data. \n","- Compare the original test images with the reconstructed images. \n","\n","**2. Visualize the Results:**\n","- Plot a few examples of original and reconstructed images side by side. \n"]},{"cell_type":"markdown","id":"51d73b43-32e7-4839-aea0-75d50f112a3f","metadata":{},"outputs":[],"source":["#### Step 5: Fine-Tuning the Autoencoder \n","\n","Fine-tuning the autoencoder by unfreezing some layers can help in improving its performance. In this exercise, you unfreeze the last four layers and train the model again for a few more epochs.\n"]},{"cell_type":"code","id":"0c39e106-a8c5-4b8a-be8c-08e5539276d4","metadata":{},"outputs":[],"source":["# Unfreeze the top layers of the encoder\nfor layer in autoencoder.layers[-4:]: \n    layer.trainable = True \n\n# Compile the model again\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy') \n\n# Train the model again\nautoencoder.fit(x_train, x_train,  \n                epochs=10,  \n                batch_size=256,  \n                shuffle=True,  \n                validation_data=(x_test, x_test))\n"]},{"cell_type":"markdown","id":"b4dee221-09e5-49c0-a573-f4e58828a561","metadata":{},"outputs":[],"source":["In the above code: \n","\n","**1. Unfreeze the Encoder Layers:**\n","- Unfreeze the last four layers of the encoder. \n","\n","**2. Compile and Train the Model:**\n","- Recompile the model. \n","- Train the model again for 10 epochs with the same training and validation data.\n"]},{"cell_type":"markdown","id":"c795f7ec-c60e-4b2e-a555-29e73affaad5","metadata":{},"outputs":[],"source":["#### Step 6: Denoising Images with Autoencoder \n","\n","In this exercise, you add random noise to the dataset and train the autoencoder to denoise the images. The autoencoder learns to reconstruct the original images from the noisy input, which can be visualized by comparing the noisy, denoised, and original images. \n"]},{"cell_type":"code","id":"553f1940-0dec-4358-8a3a-00f0988f17b2","metadata":{},"outputs":[],"source":["import numpy as np\nimport matplotlib.pyplot as plt\n\n# Add noise to the data\nnoise_factor = 0.5\nx_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\nx_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)\n\n# Train the autoencoder with noisy data\nautoencoder.fit(\n    x_train_noisy, x_train,\n    epochs=20,\n    batch_size=512,\n    shuffle=True,\n    validation_data=(x_test_noisy, x_test)\n)\n\n# Denoise the test images\nreconstructed_noisy = autoencoder.predict(x_test_noisy)\n\n# Visualize the results\nn = 10  # Number of digits to display\nplt.figure(figsize=(20, 6))\nfor i in range(n):\n    # Display noisy images\n    ax = plt.subplot(3, n, i + 1)\n    plt.imshow(x_test_noisy[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    \n    # Display denoised images\n    ax = plt.subplot(3, n, i + 1 + n)\n    plt.imshow(reconstructed_noisy[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # Display original images\n    ax = plt.subplot(3, n, i + 1 + 2 * n)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\nplt.show()\n"]},{"cell_type":"markdown","id":"6f42a487-a74b-4f2f-81e8-cc83f7dd851a","metadata":{},"outputs":[],"source":["In the above code: \n","\n","**1. Add noise to the data:**\n","- Add random noise to the training and test data. \n","- Train the Autoencoder with noisy data: \n","-Train the autoencoder using the noisy images as input and the original images as target. \n","\n","**2. Evaluate the denoising performance:**\n","- Use the autoencoder to denoise the test images. \n","- Compare the noisy, denoised, and original images. \n"]},{"cell_type":"markdown","id":"6e909751-cbe0-42f9-aa2b-1abe27c8263c","metadata":{},"outputs":[],"source":["## Practice Exercises: \n","\n","### Exercise 1: Exploring Different Bottleneck Sizes \n","\n","#### Objective: \n","\n","To understand the impact of different bottleneck sizes on the performance of the autoencoder. \n","\n","#### Instructions: \n","\n","**1. Define new models with different bottleneck sizes:**\n","- Create three new autoencoder models, each with a different bottleneck size (e.g., 16, 32, and 64 neurons). \n","- Use the same encoder and decoder architecture as in the main lab but change the number of neurons in the bottleneck layer. \n","\n","**2. Train the models:**\n","- Train each model on the MNIST dataset for 50 epochs with a batch size of 256. \n","- Use the same preprocessing steps as in the main lab. \n","\n","**3. Evaluate and Compare the Models:**\n","- Evaluate the performance of each model on the test data. \n","- Compare the reconstruction loss of the models to understand how the bottleneck size affects the autoencoder's ability to reconstruct the input data. \n"]},{"cell_type":"code","id":"2f6c38eb-7112-451e-a0e8-68b0bc402ab9","metadata":{},"outputs":[],"source":["# Write your code here"]},{"cell_type":"markdown","id":"256dc61c-0dee-404c-968f-3d6298d7fea8","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","# Define and train three different autoencoders with varying bottleneck sizes\n","bottleneck_sizes = [16, 32, 64]\n","autoencoders = []\n","\n","for size in bottleneck_sizes:\n","    # Encoder\n","    input_layer = Input(shape=(784,))\n","    encoded = Dense(64, activation='relu')(input_layer)\n","    bottleneck = Dense(size, activation='relu')(encoded)\n","\n","    # Decoder\n","    decoded = Dense(64, activation='relu')(bottleneck)\n","    output_layer = Dense(784, activation='sigmoid')(decoded)\n","\n","    # Autoencoder model\n","    autoencoder = Model(input_layer, output_layer)\n","    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n","    autoencoder.fit(\n","        x_train,\n","        x_train,\n","        epochs=20,\n","        batch_size=256,\n","        shuffle=True,\n","        validation_data=(x_test, x_test)\n","    )\n","    autoencoders.append(autoencoder)\n","\n","# Evaluate and compare the models\n","for i, size in enumerate(bottleneck_sizes):\n","    loss = autoencoders[i].evaluate(x_test, x_test)\n","    print(f'Bottleneck size {size} - Test loss: {loss}')\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"ac55ff2f-78ee-4046-babe-541a10925de5","metadata":{},"outputs":[],"source":["### Exercise 2 - Adding Regularization to the Autoencoder \n"," \n","#### Objective: \n","\n","To explore the effect of regularization on the performance of the autoencoder. \n","\n","#### Instructions: \n","\n","**1. Modify the model:**\n","- Add L2 regularization to the Dense layers in both the encoder and decoder parts of the autoencoder. \n","\n","**2. Train the model:**\n","- Train the modified autoencoder on the MNIST dataset for 50 epochs with a batch size of 256. \n","\n","**3. Evaluate and compare:**\n","- Evaluate the performance of the regularized autoencoder and compare it with the non-regularized version. \n"]},{"cell_type":"code","id":"0e337728-feee-47d7-b4bf-fb7b22f9b5a3","metadata":{},"outputs":[],"source":["# Write your code here"]},{"cell_type":"markdown","id":"318a6587-52b0-4ad2-b9d0-720e9f2f259e","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","from tensorflow.keras.regularizers import l2 \n","\n","# Encoder with L2 regularization \n","input_layer = Input(shape=(784,)) \n","encoded = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(input_layer) \n","bottleneck = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(encoded) \n","\n","# Decoder with L2 regularization \n","decoded = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(bottleneck) \n","output_layer = Dense(784, activation='sigmoid', kernel_regularizer=l2(0.01))(decoded) \n","\n","# Autoencoder model with L2 regularization \n","autoencoder_regularized = Model(input_layer, output_layer) \n","autoencoder_regularized.compile(optimizer='adam', loss='binary_crossentropy') \n","\n","# Train the model \n","autoencoder_regularized.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test, x_test)) \n","\n","# Evaluate the model \n","loss = autoencoder_regularized.evaluate(x_test, x_test) \n","print(f'Regularized Autoencoder - Test loss: {loss}') \n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"6815ff55-a513-4ff8-b896-aa0737e41d34","metadata":{},"outputs":[],"source":["### Exercise 3 - Visualizing Intermediate Representations \n","\n","#### Objective: \n","\n","To visualize and understand the intermediate representations (encoded features) learned by the autoencoder. \n","\n","#### Instructions: \n","\n","**1. Extract Encoder Part:**\n","- Extract the encoder part of the trained autoencoder to create a separate model that outputs the encoded features. \n","\n","**2. Visualize Encoded Features:**\n","- Use the encoder model to transform the test data into the encoded space. \n","- Plot the encoded features using a scatter plot for the first two dimensions of the encoded space. \n"]},{"cell_type":"code","id":"68aed269-7db0-4c4a-b69b-de45c47c2b5a","metadata":{},"outputs":[],"source":["# Writw your code here"]},{"cell_type":"markdown","id":"8c09a500-ccc2-4cf8-889d-e3e1bd0fb8fa","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","import matplotlib.pyplot as plt \n","\n","# Extract the encoder part of the autoencoder \n","encoder_model = Model(input_layer, bottleneck) \n","\n","# Encode the test data \n","encoded_imgs = encoder_model.predict(x_test) \n","\n","# Visualize the first two dimensions of the encoded features \n","plt.figure(figsize=(10, 8)) \n","plt.scatter(encoded_imgs[:, 0], encoded_imgs[:, 1], c='blue', alpha=0.5) \n","plt.title('Encoded Features - First Two Dimensions') \n","plt.xlabel('Encoded Feature 1') \n","plt.ylabel('Encoded Feature 2') \n","plt.show() \n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"f3ed256a-3801-4773-8f35-8cadd95a4130","metadata":{},"outputs":[],"source":["#### Conclusion: \n","\n","Congratulations on completing this lab! In this lab, you have gained practical experience in building, training, and evaluating autoencoders using Keras. You have learned to preprocess data, construct a basic autoencoder architecture, train the model on the MNIST dataset, and visualize the results. Additionally, you explored fine-tuning techniques to enhance the model's performance and applied the autoencoder to denoise images. \n","\n","Continue experimenting with different architectures, datasets, and applications to further deepen your knowledge and skills in using autoencoders. The concepts and techniques you have learned in this lab will serve as a foundation for more advanced topics in deep learning. \n"]},{"cell_type":"markdown","id":"b468780d-c250-4461-b204-d3658d2247ac","metadata":{},"outputs":[],"source":["## Authors\n"]},{"cell_type":"markdown","id":"04208000-bd4c-41ee-8c89-6cf91f3be138","metadata":{},"outputs":[],"source":["Skills Network\n"]},{"cell_type":"markdown","id":"2b154fbb-72aa-4ee3-8dc1-2fc25fea3344","metadata":{},"outputs":[],"source":["Copyright Â© IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"prev_pub_hash":"6b856929be8ddec6857f0588ee7cd695dfdfb707704a384fc8f032f2831a0ed5"},"nbformat":4,"nbformat_minor":4}