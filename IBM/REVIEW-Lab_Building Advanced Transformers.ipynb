{"cells":[{"cell_type":"markdown","id":"bdb08ae3-6e40-4e75-900d-da6aeb43993a","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"37fcf9da-864f-4d9e-b0da-5f995396dd17","metadata":{},"outputs":[],"source":["# **Lab: Building Advanced Transformers**\n","\n","**Estimated time needed:  30 minutes**  \n","\n","In this lab, you will implement and experiment with advanced Transformer models using Keras. \n","\n","**Learning objectives:** \n","\n","By the end of this lab, you will: \n","\n","- Implement advanced Transformer models using Keras. \n","\n","- Apply Transformers to real-world sequential data tasks. \n","\n","- Build, train, and evaluate Transformer models. \n"]},{"cell_type":"markdown","id":"f1f99b57-2c59-4862-8fac-9ff12df49057","metadata":{},"outputs":[],"source":["## Step-by-Step Instructions: \n","\n","### Step 1: Import necessary libraries \n","\n","Before you start, you need to import the required libraries: TensorFlow and Keras. Keras is included within TensorFlow as `tensorflow.keras.`\n"]},{"cell_type":"code","id":"f75f47fd-ad84-4ee2-bc2c-6311baafb7fb","metadata":{},"outputs":[],"source":["%pip install tensorflow pyarrow \n%pip install pandas  \n%pip install scikit-learn \n%pip install matplotlib \n%pip install requests\n\n"]},{"cell_type":"code","id":"f7b493f8-9f56-4ea0-97cf-bb6c4f3ebfab","metadata":{},"outputs":[],"source":["import numpy as np \nimport pandas as pd \nimport tensorflow as tf \nimport requests\nfrom sklearn.preprocessing import MinMaxScaler \nfrom tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout"]},{"cell_type":"markdown","id":"84a6fc6c-1977-442d-a622-0794062257e1","metadata":{},"outputs":[],"source":["####  Setup the Environment to generate synthetic stock price data\n"]},{"cell_type":"code","id":"f7a1644f-1983-4208-823e-1923cc2be243","metadata":{},"outputs":[],"source":["import numpy as np\nimport pandas as pd\n\n# Create a synthetic stock price dataset\nnp.random.seed(42)\ndata_length = 2000  # Adjust data length as needed\ntrend = np.linspace(100, 200, data_length)\nnoise = np.random.normal(0, 2, data_length)\nsynthetic_data = trend + noise\n\n# Create a DataFrame and save as 'stock_prices.csv'\ndata = pd.DataFrame(synthetic_data, columns=['Close'])\ndata.to_csv('stock_prices.csv', index=False)\nprint(\"Synthetic stock_prices.csv created and loaded.\")\n"]},{"cell_type":"code","id":"0b9f0a73-eb0e-4c0b-adb1-2f068a5f33d7","metadata":{},"outputs":[],"source":["# Load the dataset \ndata = pd.read_csv('stock_prices.csv') \ndata = data[['Close']].values \n\n# Normalize the data\nscaler = MinMaxScaler(feature_range=(0, 1))\ndata = scaler.fit_transform(data)\n\n# Prepare the data for training\ndef create_dataset(data, time_step=1):\n    X, Y = [], []\n\n    for i in range(len(data)-time_step-1):\n        a = data[i:(i+time_step), 0]\n        X.append(a)\n        Y.append(data[i + time_step, 0])\n    return np.array(X), np.array(Y)\n\ntime_step = 100\nX, Y = create_dataset(data, time_step)\nX = X.reshape(X.shape[0], X.shape[1], 1)\n\nprint(\"Shape of X:\", X.shape) \nprint(\"Shape of Y:\", Y.shape) "]},{"cell_type":"markdown","id":"0b218abb-d401-4727-a317-2e381c2d1103","metadata":{},"outputs":[],"source":["In the above code: \n","\n","`tensorflow` is the main library for machine learning in Python.  \n","\n","`stock_prices.csv` is the data set that is loaded. \n","\n","`MinMaxScaler` method is used to normalize the data.  \n","\n","`create_dataset`method is used to prepare the data for training. \n"]},{"cell_type":"markdown","id":"834533dc-d545-4225-abe5-bf6702a63b29","metadata":{},"outputs":[],"source":["### Step 2: Implement Multi-Head Self-Attention \n","\n","Define the Multi-Head Self-Attention mechanism. \n"]},{"cell_type":"code","id":"c17e005d-bb96-4e35-84f6-1b64ff95198f","metadata":{},"outputs":[],"source":["class MultiHeadSelfAttention(Layer): \n\n    def __init__(self, embed_dim, num_heads=8): \n        super(MultiHeadSelfAttention, self).__init__() \n        self.embed_dim = embed_dim \n        self.num_heads = num_heads \n        self.projection_dim = embed_dim // num_heads \n        self.query_dense = Dense(embed_dim) \n        self.key_dense = Dense(embed_dim) \n        self.value_dense = Dense(embed_dim) \n        self.combine_heads = Dense(embed_dim) \n\n\n    def attention(self, query, key, value): \n        score = tf.matmul(query, key, transpose_b=True) \n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n        scaled_score = score / tf.math.sqrt(dim_key) \n        weights = tf.nn.softmax(scaled_score, axis=-1) \n        output = tf.matmul(weights, value) \n        return output, weights \n\n    def split_heads(self, x, batch_size): \n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n        return tf.transpose(x, perm=[0, 2, 1, 3]) \n\n    def call(self, inputs): \n        batch_size = tf.shape(inputs)[0] \n        query = self.query_dense(inputs) \n        key = self.key_dense(inputs) \n        value = self.value_dense(inputs) \n        query = self.split_heads(query, batch_size) \n        key = self.split_heads(key, batch_size) \n        value = self.split_heads(value, batch_size) \n        attention, _ = self.attention(query, key, value) \n        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n        output = self.combine_heads(concat_attention) \n        return output \n\n "]},{"cell_type":"markdown","id":"9559b2f1-6474-4761-b3e1-e8d463be0b80","metadata":{},"outputs":[],"source":["In the above code: \n","\n","- The MultiHeadSelfAttention layer implements the multi-head self-attention mechanism, which allows the model to focus on different parts of the input sequence simultaneously. \n","\n","- The attention parameter computes the attention scores and weighted sum of the values. \n","\n","- The split_heads parameter splits the input into multiple heads for parallel attention computation. \n","\n","- The call method applies the self-attention mechanism and combines the heads. \n"]},{"cell_type":"markdown","id":"19eacfd8-cf47-49e2-b3d6-37f4a9f7fc91","metadata":{},"outputs":[],"source":["### Step 3: Implement Transformer block \n","\n","Define the Transformer block. \n"]},{"cell_type":"code","id":"8d98b16c-1273-47db-a7c1-1b86c156f923","metadata":{},"outputs":[],"source":["class TransformerBlock(Layer): \n\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n        super(TransformerBlock, self).__init__() \n        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n        self.ffn = tf.keras.Sequential([ \n            Dense(ff_dim, activation=\"relu\"), \n            Dense(embed_dim), \n        ]) \n\n        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n        self.dropout1 = Dropout(rate) \n        self.dropout2 = Dropout(rate) \n\n\n    def call(self, inputs, training): \n        attn_output = self.att(inputs) \n        attn_output = self.dropout1(attn_output, training=training) \n        out1 = self.layernorm1(inputs + attn_output) \n        ffn_output = self.ffn(out1) \n        ffn_output = self.dropout2(ffn_output, training=training) \n        return self.layernorm2(out1 + ffn_output) "]},{"cell_type":"markdown","id":"59ac1a9c-a6fd-426a-8150-7d73bbda0260","metadata":{},"outputs":[],"source":["In the above code:\n","\n","- The TransformerBlock layer combines multi-head self-attention with a feed-forward neural network and normalization layers.  \n","\n","- Dropout is used to prevent overfitting. \n","\n","- The call method applies the self-attention, followed by the feedforward network with residual connections and layer normalization.\n"]},{"cell_type":"markdown","id":"44236ed7-6e90-4272-8ddb-0b23f162e801","metadata":{},"outputs":[],"source":["### Step 4: Implement Encoder Layer \n","\n","Define the Encoder layer. \n"]},{"cell_type":"code","id":"6ae62188-09fc-4efa-be57-4ccdc7388d06","metadata":{},"outputs":[],"source":["class EncoderLayer(Layer): \n\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n        super(EncoderLayer, self).__init__() \n        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n        self.ffn = tf.keras.Sequential([ \n            Dense(ff_dim, activation=\"relu\"), \n            Dense(embed_dim), \n        ]) \n\n        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n        self.dropout1 = Dropout(rate) \n        self.dropout2 = Dropout(rate) \n\n \n\n    def call(self, inputs, training): \n        attn_output = self.att(inputs) \n        attn_output = self.dropout1(attn_output, training=training) \n        out1 = self.layernorm1(inputs + attn_output) \n        ffn_output = self.ffn(out1) \n        ffn_output = self.dropout2(ffn_output, training=training) \n        return self.layernorm2(out1 + ffn_output) \n\n"]},{"cell_type":"markdown","id":"f9c4a011-d544-467e-8dd4-5b785a226924","metadata":{},"outputs":[],"source":["In the above code: \n","\n","- The EncoderLayer is similar to the TransformerBlock but is a reusable layer in the Transformer architecture. \n","\n","- It consists of a MultiHeadSelfAttention mechanism followed by a feedforward neural network. \n","\n","- Both sub-layers have residual connections around them, and layer normalization is applied to the output of each sub-layer. \n","\n","- The call method applies the self-attention, followed by the feedforward network, with residual connections and layer normalization. \n"]},{"cell_type":"markdown","id":"d4d804e1-689b-4876-be82-6a65a1381154","metadata":{},"outputs":[],"source":["### Step 5: Implement Transformer encoder \n","\n","Define the Transformer Encoder. \n"]},{"cell_type":"code","id":"d4cc36bd-6bd8-4334-8571-d3ec5a17c69e","metadata":{},"outputs":[],"source":["import tensorflow as tf \nfrom tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout \n\nclass MultiHeadSelfAttention(Layer): \n    def __init__(self, embed_dim, num_heads=8): \n        super(MultiHeadSelfAttention, self).__init__() \n        self.embed_dim = embed_dim \n        self.num_heads = num_heads \n        self.projection_dim = embed_dim // num_heads \n        self.query_dense = Dense(embed_dim) \n        self.key_dense = Dense(embed_dim) \n        self.value_dense = Dense(embed_dim) \n        self.combine_heads = Dense(embed_dim) \n \n\n    def attention(self, query, key, value): \n        score = tf.matmul(query, key, transpose_b=True) \n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n        scaled_score = score / tf.math.sqrt(dim_key) \n        weights = tf.nn.softmax(scaled_score, axis=-1) \n        output = tf.matmul(weights, value) \n        return output, weights \n\n\n    def split_heads(self, x, batch_size): \n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n        return tf.transpose(x, perm=[0, 2, 1, 3]) \n\n\n    def call(self, inputs): \n        batch_size = tf.shape(inputs)[0] \n        query = self.query_dense(inputs) \n        key = self.key_dense(inputs) \n        value = self.value_dense(inputs) \n        query = self.split_heads(query, batch_size) \n        key = self.split_heads(key, batch_size) \n        value = self.split_heads(value, batch_size) \n        attention, _ = self.attention(query, key, value) \n        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n        output = self.combine_heads(concat_attention) \n        return output \n\nclass TransformerBlock(Layer): \n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n        super(TransformerBlock, self).__init__() \n        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n        self.ffn = tf.keras.Sequential([ \n            Dense(ff_dim, activation=\"relu\"), \n            Dense(embed_dim), \n        ]) \n\n        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n        self.dropout1 = Dropout(rate) \n        self.dropout2 = Dropout(rate) \n \n\n    def call(self, inputs, training): \n        attn_output = self.att(inputs) \n        attn_output = self.dropout1(attn_output, training=training) \n        out1 = self.layernorm1(inputs + attn_output) \n        ffn_output = self.ffn(out1) \n        ffn_output = self.dropout2(ffn_output, training=training) \n        return self.layernorm2(out1 + ffn_output) \n\nclass TransformerEncoder(Layer): \n    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, rate=0.1): \n        super(TransformerEncoder, self).__init__() \n        self.num_layers = num_layers \n        self.embed_dim = embed_dim \n        self.enc_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)] \n        self.dropout = Dropout(rate) \n\n    def call(self, inputs, training=False): \n        x = inputs \n        for i in range(self.num_layers): \n            x = self.enc_layers[i](x, training=training) \n        return x \n\n# Example usage \nembed_dim = 128 \nnum_heads = 8 \nff_dim = 512 \nnum_layers = 4 \n\ntransformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \ninputs = tf.random.uniform((1, 100, embed_dim)) \noutputs = transformer_encoder(inputs, training=False)  # Use keyword argument for 'training' \nprint(outputs.shape)  # Should print (1, 100, 128) "]},{"cell_type":"markdown","id":"58914cf7-70fa-4e3a-9a13-2c7ea907f91e","metadata":{},"outputs":[],"source":["In the above code: \n","\n","The TransformerEncoder is composed of multiple TransformerBlock layers, implementing the encoding part of the Transformer architecture. \n"]},{"cell_type":"markdown","id":"ade3a268-1398-489b-965b-63d7cb4f70b9","metadata":{},"outputs":[],"source":["### Step 6: Build and Compile the Transformer model \n","\n","Integrate the Transformer Encoder into a complete model for sequential data. \n"]},{"cell_type":"code","id":"973dc690-4c2f-4edf-aa69-63be850f3ece","metadata":{},"outputs":[],"source":["# Define the necessary parameters \n\nembed_dim = 128 \nnum_heads = 8 \nff_dim = 512 \nnum_layers = 4 \n\n# Define the Transformer Encoder \ntransformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n\n# Build the model \ninput_shape = (X.shape[1], X.shape[2]) \ninputs = tf.keras.Input(shape=input_shape) \n\n# Project the inputs to the embed_dim \nx = tf.keras.layers.Dense(embed_dim)(inputs) \nencoder_outputs = transformer_encoder(x) \nflatten = tf.keras.layers.Flatten()(encoder_outputs) \noutputs = tf.keras.layers.Dense(1)(flatten) \nmodel = tf.keras.Model(inputs, outputs) \n\n# Compile the model \nmodel.compile(optimizer='adam', loss='mse') \n\n# Summary of the model \nmodel.summary() \n"]},{"cell_type":"markdown","id":"51fb9cf5-9372-4794-add8-5f2392838a23","metadata":{},"outputs":[],"source":["In the above code: \n","\n","- The Transformer Encoder model defines the necessary parameters, flattens the output, and ends with a dense layer to produce the final output.  \n","\n","- The model is then compiled with the Adam optimizer and mean squared error loss. \n"]},{"cell_type":"markdown","id":"e5978fb3-3a42-44f9-b146-68cad41ba794","metadata":{},"outputs":[],"source":["### Step 7: Train the Transformer model \n","\n","Train the model on the prepared dataset. \n"]},{"cell_type":"code","id":"a65d7244-5b68-4ba7-9f94-27cb0022e768","metadata":{},"outputs":[],"source":["# Train the model\nmodel.fit(X, Y, epochs=20, batch_size=32)\n"]},{"cell_type":"markdown","id":"6aa04758-fc9b-4fb2-b00e-535326f274a6","metadata":{},"outputs":[],"source":["In the above code: \n","\n","The model is trained on the normalized stock price data for 20 epochs with a batch size of 32. \n"]},{"cell_type":"markdown","id":"98c73638-a697-414f-91b4-3cf1455015db","metadata":{},"outputs":[],"source":["### Step 8: Evaluate and Make Predictions \n","\n","Evaluate the model's performance and make predictions on the dataset. \n"]},{"cell_type":"code","id":"14dc0b3a-758e-407e-b392-cd3b9a1c2fa1","metadata":{},"outputs":[],"source":["# Make predictions \npredictions = model.predict(X) \npredictions = scaler.inverse_transform(predictions) \n \n\n# Plot the predictions \nimport matplotlib.pyplot as plt \n\nplt.plot(data, label='True Data') \nplt.plot(np.arange(time_step, time_step + len(predictions)), predictions, label='Predictions') \nplt.xlabel('Time') \nplt.ylabel('Stock Price') \nplt.legend() \nplt.show() \n\n "]},{"cell_type":"markdown","id":"7654ac42-8a22-4304-b257-2d344dcd360f","metadata":{},"outputs":[],"source":["In the above code: \n","\n","- The model's predictions are transformed back to the original scale using the inverse transform of the scaler. \n","\n","- The true data and predictions are plotted to visualize the model's performance. \n"]},{"cell_type":"markdown","id":"0c9e038e-2bd0-48df-a073-317143f34a65","metadata":{},"outputs":[],"source":["## Practice Exercises: \n","\n"," ### Exercise 1: Add dropout to the Transformer model \n","\n"," **Objective: Understand how to add dropout layers to the Transformer model to prevent overfitting.** \n","\n"," Instructions: \n","\n","- Add a dropout layer after the Flatten layer in the model. \n","\n","- Set the dropout rate to 0.5. \n"]},{"cell_type":"code","id":"658814d0-81f8-4e42-9196-f58a8ba73174","metadata":{},"outputs":[],"source":["## Write your code here.\n"]},{"cell_type":"markdown","id":"8b631728-90c3-4045-9a46-4981381b9f26","metadata":{},"outputs":[],"source":["<details><summary>Click here to view the solution.</summary>\n","\n","```\n","from tensorflow.keras.layers import Dropout \n","\n","  \n","\n","# Add a dropout layer after the Flatten layer \n","\n","flatten = tf.keras.layers.Flatten()(encoder_outputs) \n","\n","dropout = Dropout(0.5)(flatten) \n","\n","outputs = tf.keras.layers.Dense(1)(dropout) \n","\n","  \n","\n","# Build the model \n","\n","model = tf.keras.Model(inputs, outputs) \n","\n","  \n","\n","# Compile the model \n","\n","model.compile(optimizer='adam', loss='mse') \n","\n","  \n","\n","# Train the model \n","\n","model.fit(X, Y, epochs=20, batch_size=32) \n","\n","  \n","\n","# Evaluate the model \n","\n","loss = model.evaluate(X, Y) \n","\n","print(f'Test loss: {loss}') \n","\n","```\n","</details>\n"]},{"cell_type":"markdown","id":"9ac4285e-8886-47e6-8946-e10493394914","metadata":{},"outputs":[],"source":["### Exercise 2: Experiment with different batch sizes \n","\n","**Objective: Observe the impact of different batch sizes on model performance.** \n","\n"," Instructions: \n","\n","- Train the model with a batch size of 16. \n","\n","- Train the model with a batch size of 64. \n","\n","- Compare the training time and performance. \n"]},{"cell_type":"code","id":"960017cb-8c0e-4d60-9447-81f4be936add","metadata":{},"outputs":[],"source":["## Write your code here.\n"]},{"cell_type":"markdown","id":"32f752d7-fece-4267-9164-9a64bcae3911","metadata":{},"outputs":[],"source":["<details><summary>Click here to view the solution.</summary>\n","\n","```\n","# Train the model with batch size 16\n","model.fit(X, Y, epochs=20, batch_size=16)\n","\n","# Evaluate the model\n","loss = model.evaluate(X, Y)\n","print(f'Test loss with batch size 16: {loss}')\n","\n","# Train the model with batch size 64\n","model.fit(X, Y, epochs=20, batch_size=64)\n","\n","# Evaluate the model\n","loss = model.evaluate(X, Y)\n","print(f'Test loss with batch size 64: {loss}')\n","\n","```\n","</details>\n"]},{"cell_type":"markdown","id":"359aec66-7a1c-4f1b-9be3-6a7f90905c3d","metadata":{},"outputs":[],"source":["### Exercise 3: Use a different activation function \n","\n"," **Objective: Understand how different activation functions impact the model performance.** \n","\n"," Instructions: \n","\n","- Change the activation function of the Dense layer to `tanh`. \n","\n","- Train and evaluate the model. \n"]},{"cell_type":"code","id":"194078fc-6b2a-4543-8200-b3d6c9e71e57","metadata":{},"outputs":[],"source":["## Write your code here.\n"]},{"cell_type":"markdown","id":"ade7ca29-e106-4a76-97b6-5fd814a01a0e","metadata":{},"outputs":[],"source":["<details><summary>Click here to view the solution.</summary>\n","\n","```\n","# Change the activation function of the Dense layer to tanh\n","outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n","\n","# Build the model\n","model = tf.keras.Model(inputs, outputs)\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='mse')\n","\n","# Train the model\n","model.fit(X, Y, epochs=20, batch_size=32)\n","\n","# Evaluate the model\n","loss = model.evaluate(X, Y)\n","print(f'Test loss with tanh activation: {loss}')\n","\n","```\n","</details>\n"]},{"cell_type":"markdown","id":"27b057d6-dab5-465e-bfc2-a2e1238297ad","metadata":{},"outputs":[],"source":["## Conclusion\n","Congratulations on completing this lab! In this lab, you have built an advanced Transformer model using Keras and applied it to a time series forecasting task. You have learned how to define and implement multi-head self-attention, Transformer blocks, encoder layers, and integrate them into a complete Transformer model. By experimenting with different configurations and training the model, you can further improve its performance and apply it to various sequential data tasks. \n"]},{"cell_type":"markdown","id":"d130ec79-66d4-4f13-9e97-f5d0508ec412","metadata":{},"outputs":[],"source":["Copyright © IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"prev_pub_hash":"28ac4fd81c1d713f83dcd1cdf1d3383ad25ea92873288fe9e978e9a17b314709"},"nbformat":4,"nbformat_minor":4}