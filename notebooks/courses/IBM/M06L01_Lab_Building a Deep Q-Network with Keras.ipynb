{"cells":[{"cell_type":"markdown","id":"a9c546c9-dcf7-4910-999a-e489945c7634","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"7f115b78-065a-42e8-8305-647702730121","metadata":{},"outputs":[],"source":["# **Lab: Building a Deep Q-Network with Keras**\n"]},{"cell_type":"markdown","id":"dc07093c-ddbb-437f-b135-c91f14d5f50e","metadata":{},"outputs":[],"source":["Estimated time needed: **30** minutes\n"]},{"cell_type":"markdown","id":"c6c19d95-98ef-4283-94ba-bddea8e0205b","metadata":{},"outputs":[],"source":["In this lab, you will implement a Deep Q-Network (DQN) using Keras to solve a reinforcement learning problem. You will set up the environment, define the DQN model, train the agent, and evaluate its performance. By the end of this lab, you will know how to apply DQNs in Keras to optimize action-selection policies for a given environment. \n"]},{"cell_type":"markdown","id":"51acf721-b5b3-4969-8fe1-347470871294","metadata":{},"outputs":[],"source":["## Learning objectives \n","By the end of this lab, you will: \n","- Implement a Deep Q-Network using Keras  \n","- Define and train a neural network to approximate the Q-values  \n","- Evaluate the performance of the trained DQN agent \n","\n","## Prerequisites \n","- Basic understanding of Python and Keras \n","- Familiarity with neural networks \n","- Understanding of reinforcement learning concepts \n"]},{"cell_type":"markdown","id":"5b473ec5-34a0-401c-aa14-e1dc20c02b48","metadata":{},"outputs":[],"source":["### Steps \n","\n","#### Step 1: Set up the environment \n","\n","Before you start, set up the environment using the OpenAI Gym library. You will use the 'CartPole-v1' environment, which is a common benchmark for reinforcement learning algorithms. \n","\n"," \n"]},{"cell_type":"code","id":"cff81e42-7d40-49ab-bfe4-b791d9fd7946","metadata":{},"outputs":[],"source":["!pip install gym"]},{"cell_type":"code","id":"922d2cb6-4819-4f9c-a40d-43aa5a52141b","metadata":{},"outputs":[],"source":["!pip install tensorflow==2.16.2"]},{"cell_type":"code","id":"2cad454d-b4ba-449d-b8b8-b2146424269b","metadata":{},"outputs":[],"source":["import gym\nimport numpy as np\n\n# Create the environment  \nenv = gym.make('CartPole-v1')\n\n# Set random seed for reproducibility  \nnp.random.seed(42)\nenv.reset(seed=42)"]},{"cell_type":"markdown","id":"a79744a6-ce74-4a1a-a11b-05218dbe9798","metadata":{},"outputs":[],"source":["#### Notes: \n","- `gym` is a toolkit for developing and comparing reinforcement learning algorithms.\n","- `CartPole-v1` is an environment where a pole is balanced on a cart, and the goal is to prevent the pole from falling over.  \n","- Setting random seeds ensures that you can reproduce the results.\n"]},{"cell_type":"markdown","id":"afafe069-acac-46e1-8c2e-da00d4fe69f7","metadata":{},"outputs":[],"source":["#### Step 2: Define the DQN model \n","\n","Define a neural network using Keras to approximate the Q-values. The network will take the state as input and output Q-values for each action. \n"]},{"cell_type":"code","id":"2bdd2bb4-7257-453d-aa1d-4bb5807d5897","metadata":{},"outputs":[],"source":["# Suppress warnings for a cleaner notebook or console experience\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Disable warnings for a cleaner notebook or console experience\ndef warn(*args, **kwargs):\n    pass\nwarnings.warn = warn\n\n# Import necessary libraries\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\n\n\ndef build_model(state_size, action_size):\n    model = Sequential()\n    model.add(Dense(24, input_dim=state_size, activation='relu'))\n    model.add(Dense(24, activation='relu'))\n    model.add(Dense(action_size, activation='linear'))\n    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n    return model\n\nstate_size = env.observation_space.shape[0]\naction_size = env.action_space.n\nmodel = build_model(state_size, action_size)"]},{"cell_type":"markdown","id":"27957a3a-c8d8-414c-9b04-d0d19f3d1864","metadata":{},"outputs":[],"source":["#### Notes: \n","\n","- `Sequential` model: a linear stack of layers in Keras. \n","- `Dense` layers: fully connected layers.  \n","- `input_dim`: the size of the input layer, corresponding to the state size.  \n","- `activation='relu'`: Rectified Linear Unit activation function.  \n","- `activation='linear'`: linear activation function for the output layer. \n","- `Adam` optimizer: an optimization algorithm that adjusts the learning rate based on gradients.\n"]},{"cell_type":"markdown","id":"5f56a25b-40dc-462b-86ae-e92cccbf32d7","metadata":{},"outputs":[],"source":["#### Step 3: Implement the replay buffer \n","\n","A replay buffer stores the agent's experiences for training. We will implement a replay buffer using a deque. \n","\n"," \n"]},{"cell_type":"code","id":"5631e918-4d87-437c-ac8f-45e074b13aff","metadata":{},"outputs":[],"source":["from collections import deque\nimport random\n\nmemory = deque(maxlen=2000)\ndef remember(state, action, reward, next_state, done):\n    memory.append((state, action, reward, next_state, done))"]},{"cell_type":"markdown","id":"cd0e2c27-56f7-4b3d-b778-286af3f94d37","metadata":{},"outputs":[],"source":["#### Notes: \n","\n","- `memory`: a deque to store experiences (state, action, reward, next_state, done).  \n","- `remember()`: stores experiences in memory.\n"]},{"cell_type":"markdown","id":"29796f0f-640f-431e-9c81-faeea838f6f3","metadata":{},"outputs":[],"source":["#### Step 4: Implement the epsilon-greedy policy \n","\n","The epsilon-greedy policy balances exploration and exploitation by choosing random actions with probability epsilon. \n"]},{"cell_type":"code","id":"79d422a6-688f-4822-8016-806e5e63c6bf","metadata":{},"outputs":[],"source":["epsilon = 1.0\nepsilon_min = 0.01\nepsilon_decay = 0.995\n \ndef act(state):\n    if np.random.rand() <= epsilon:\n        return random.randrange(action_size)\n    q_values = model.predict(state)\n    return np.argmax(q_values[0])"]},{"cell_type":"markdown","id":"0b2ecd53-77ac-42ad-8520-aa53503c8df4","metadata":{},"outputs":[],"source":["#### Notes: \n","- `epsilon`: exploration rate.  \n","- `epsilon_min`: minimum exploration rate.  \n","- `epsilon_decay`: decay rate for epsilon after each episode.  \n","- `act()`: chooses an action based on the epsilon-greedy policy.\n"]},{"cell_type":"markdown","id":"3b78feb2-79ba-4f34-b224-8a78f0883006","metadata":{},"outputs":[],"source":["#### Step 5: Implement the Q-learning update \n","\n","Implement the Q-learning update to train the DQN using experiences stored in the replay buffer. \n"]},{"cell_type":"code","id":"966962dc-2cde-4552-82de-7f22586c52ee","metadata":{},"outputs":[],"source":["def replay(batch_size):\n    global epsilon\n    minibatch = random.sample(memory, batch_size)\n    for state, action, reward, next_state, done in minibatch:\n        target = reward\n        if not done:\n            target = reward + gamma * np.amax(model.predict(next_state)[0])\n        target_f = model.predict(state)\n        target_f[0][action] = target\n        model.fit(state, target_f, epochs=1, verbose=0)\n    if epsilon > epsilon_min:\n        epsilon *= epsilon_decay"]},{"cell_type":"markdown","id":"7431dcd6-6c49-47ee-90ec-1fc11560d85a","metadata":{},"outputs":[],"source":["#### Notes: \n","- `replay()`: samples a random minibatch from memory and trains the model.  \n","- `target`: the Q-value target, which is updated using the reward and the maximum Q-value of the next state.  \n","- `model.fit()`: trains the model on the updated Q-values.\n"]},{"cell_type":"markdown","id":"dfb6d6d1-17db-4e33-8641-e5a4a2cf25b8","metadata":{},"outputs":[],"source":["#### Step 6: Train the DQN \n","\n","Train the DQN agent by interacting with the environment and updating the Q-values using the replay buffer.\n"]},{"cell_type":"code","id":"12c52974-207b-40cf-a38c-133cc4525523","metadata":{},"outputs":[],"source":["# Training loop\nepisodes = 50  # More episodes to ensure sufficient training\nbatch_size = 32  # Mini-batch size for replay training\ngamma = 0.95  # Discount factor for future rewards\n \nfor e in range(episodes):\n    state = env.reset()\n    if isinstance(state, tuple):  # Handle tuple output\n        state = state[0]\n    state = np.reshape(state, [1, state_size])\n \n    for time in range(200):  # Max steps per episode\n        # Choose action using epsilon-greedy policy\n        action = act(state)\n \n        # Perform action in the environment\n        result = env.step(action)\n        if len(result) == 4:  # Handle 4-value output\n            next_state, reward, done, _ = result\n        else:  # Handle 5-value output\n            next_state, reward, done, _, _ = result\n \n        if isinstance(next_state, tuple):  # Handle tuple next_state\n            next_state = next_state[0]\n        next_state = np.reshape(next_state, [1, state_size])\n \n        # Store experience in memory\n        remember(state, action, reward, next_state, done)\n \n        # Update state\n        state = next_state\n \n        if done:  # If episode ends\n            print(f\"Episode: {e+1}/{episodes}, Score: {time}, Epsilon: {epsilon:.2}\")\n            break\n \n    # Train the model using replay memory\n    if len(memory) > batch_size:\n        replay(batch_size)\n \nenv.close()"]},{"cell_type":"markdown","id":"6687b141-3020-401f-acc0-2fd594ee6e60","metadata":{},"outputs":[],"source":["#### Notes: \n","- The main loop iterates over episodes, interacting with the environment and training the model.  \n","- `env.reset()`: resets the environment at the beginning of each episode.  \n","- `env.step(action)`: takes the chosen action and observes the reward and next state.  \n","- The score for each episode is printed to monitor training progress.\n"]},{"cell_type":"markdown","id":"c09a1f4d-26f3-473d-8828-1a8ba4180000","metadata":{},"outputs":[],"source":["#### Step 7: Evaluate the performance \n","\n","Evaluate the performance of the trained DQN agent. \n"]},{"cell_type":"code","id":"6171e2ee-fcdc-4ad7-b7b7-079683bdef8f","metadata":{},"outputs":[],"source":["# Evaluation loop\nevaluation_episodes = 10  # Number of evaluation episodes\nscores = []  # Track scores for performance metrics\n \nfor e in range(evaluation_episodes):\n    state = env.reset()\n    if isinstance(state, tuple):  # Handle tuple output\n        state = state[0]\n    state = np.reshape(state, [1, state_size])\n \n    total_reward = 0  # Track total reward per episode\n \n    for time in range(200):  # Max steps per episode\n        # Choose the greedy action\n        action = np.argmax(model.predict(state)[0])\n \n        # Perform action in the environment\n        result = env.step(action)\n        if len(result) == 4:  # Handle 4-value output\n            next_state, reward, done, _ = result\n        else:  # Handle 5-value output\n            next_state, reward, done, _, _ = result\n \n        if isinstance(next_state, tuple):  # Handle tuple next_state\n            next_state = next_state[0]\n        next_state = np.reshape(next_state, [1, state_size])\n \n        state = next_state\n        total_reward += reward\n \n        if done:  # If episode ends\n            print(f\"Evaluation Episode: {e+1}/{evaluation_episodes}, Score: {time}, Total Reward: {total_reward}\")\n            scores.append(total_reward)\n            break\n \n# Summary of evaluation performance\nprint(f\"Average Reward: {np.mean(scores):.2f}, Max Reward: {np.max(scores)}, Min Reward: {np.min(scores)}\")\n \nenv.close()"]},{"cell_type":"markdown","id":"0c36773c-1690-46b1-8a98-2a7bfa1b9c66","metadata":{},"outputs":[],"source":["#### Notes:\n","- This loop runs 10 episodes to test the trained agent.\n","- `env.render()`: Visualizes the environment.\n","- The agent chooses actions based on the trained model and interacts with the environment.\n","- The environment's `reset` and `step` methods may return additional information in the form of tuples. The code now checks if the `state` and `next_state` are tuples and extracts the necessary data.\n","- The `env.step(action)` method may return more than the standard four values. The code has been updated to handle these additional values by unpacking only the necessary information and ignoring the rest.\n","- The score for each episode is printed, indicating how long the agent was able to balance the pole in each episode.\n"]},{"cell_type":"markdown","id":"b04d704d-aba4-4dae-846a-e165f777850b","metadata":{},"outputs":[],"source":["## Practice Exercises\n","### Exercise 1: Modify the Reward Function to Encourage Longer Episodes\n","**Objective:** Modify the reward structure to encourage the agent to keep the pole balanced longer.\n","\n","**Instructions:**\n","1. Instead of just using the environment's reward, modify the reward function to include a penalty for large pole angles.\n","2. Update the reward calculation in the `train_dqn` function to discourage the agent from letting the pole deviate too far from the center.\n","3. Observe the effect on the agent's learning and episode length.\n"]},{"cell_type":"code","id":"ea2cd517-6d3b-4478-9307-8854690ae7cf","metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"338867dd-58b6-4e74-a3b3-da71baa0ab6f","metadata":{},"outputs":[],"source":["<details>\n","<summary>Click here for solution</summary> </br>\n","\n","```python\n","import os\n","\n","# Create sample directory structure if it does not exist\n","base_dir = 'sample_data'\n","train_dir = os.path.join(base_dir, 'train')\n","val_dir = os.path.join(base_dir, 'validation')\n","class1_train = os.path.join(train_dir, 'class1')\n","class2_train = os.path.join(train_dir, 'class2')\n","class1_val = os.path.join(val_dir, 'class1')\n","class2_val = os.path.join(val_dir, 'class2')\n","\n","# Create directories if they do not exist\n","for dir_path in [train_dir, val_dir, class1_train, class2_train, class1_val, class2_val]:\n","    os.makedirs(dir_path, exist_ok=True)\n","\n","print(\"Directory structure created. Add your images to these directories.\")\n","\n","# Import the necessary library\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Modify data generator to include validation data\n","train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n","\n","train_generator = train_datagen.flow_from_directory(\n","    'sample_data',\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='binary',\n","    subset='training'\n",")\n","\n","validation_generator = train_datagen.flow_from_directory(\n","    'sample_data',\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='binary',\n","    subset='validation'\n",")\n","\n","# Function to modify the reward to encourage longer episodes\n","def modify_reward(reward, next_state):\n","    # Penalize large pole angles\n","    pole_angle = abs(next_state[2])  # Extract the pole angle from the state\n","    penalty = 1 if pole_angle > 0.1 else 0  # Apply penalty if angle is large\n","    return reward - penalty  # Adjust reward\n","\n","# Inside the training loop\n","# Example usage in a reinforcement learning training loop:\n","# reward = modify_reward(reward, next_state)  # Use the modified reward\n","\n"]},{"cell_type":"markdown","id":"9ef1506b-42fc-4fe5-9c6e-e74c81527e9e","metadata":{},"outputs":[],"source":["### Exercise 2: Implement Early Stopping Based on Episode Length\n","**Objective:** Stop training early if the agent consistently reaches the maximum episode length.\n","\n","**Instructions:**\n","1. Add an early stopping mechanism that stops training if the agent achieves a specified number of consecutive episodes with a length above a threshold.\n","2. Set the threshold as 195 steps for 100 consecutive episodes (for CartPole).\n","3. Print a message and stop training when this condition is met.\n"]},{"cell_type":"code","id":"213d2bc3-7ffb-4fea-93f7-ee722e8ec6ef","metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"e25259b7-17b7-45e3-a3d5-62a3c4e04664","metadata":{},"outputs":[],"source":["<details>\n","<summary>Click here for solution</summary> </br>\n","\n","```python\n","# Import necessary libraries\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Modify data generator to include validation data\n","train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n","\n","train_generator = train_datagen.flow_from_directory(\n","    'sample_data',  \n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='binary',\n","    subset='training'\n",")\n","\n","validation_generator = train_datagen.flow_from_directory(\n","    'sample_data',  \n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='binary',\n","    subset='validation'\n",")\n","\n","# Early stopping parameters\n","consecutive_success_threshold = 100\n","success_episode_length = 195\n","consecutive_success_count = 0\n","episode_lengths = []  # Initialize episode lengths list\n","\n","# Example of training loop (this should be your actual loop)\n","for episode in range(1000):  # Replace with actual loop condition\n","    # Training logic goes here\n","    episode_length = 200  # Example value, replace with actual calculation\n","    episode_lengths.append(episode_length)\n","    \n","    # Early stopping check\n","    if len(episode_lengths) > consecutive_success_threshold and all(\n","        length >= success_episode_length for length in episode_lengths[-consecutive_success_threshold:]\n","    ):\n","        print(\"Early stopping: Agent consistently reaches max episode length.\")\n","        break  # This break is now correctly inside the loop\n"]},{"cell_type":"markdown","id":"e8e84e67-0280-4554-8c01-b7ea2fa9a141","metadata":{},"outputs":[],"source":["### Exercise 3: Experiment with Different Exploration Strategies\n","**Objective:** Implement an epsilon decay schedule that switches from linear decay to exponential decay after a certain number of episodes.\n","\n","**Instructions:**\n","1. Modify the epsilon decay strategy to start with a linear decay until a specific episode, then switch to an exponential decay.\n","2. Implement a check in the `choose_action` function to change the decay strategy after 100 episodes.\n","3. Observe and compare the agent’s performance.\n"]},{"cell_type":"code","id":"6a6622c8-1de4-4c35-b469-d42370029204","metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"c0aadfbe-0a56-4cfb-8ba7-2d200ff1625b","metadata":{},"outputs":[],"source":["<details>\n","<summary>Click here for solution</summary> </br>\n","\n","```python\n","# Modify data generator to include validation data\n","train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n","\n","train_generator = train_datagen.flow_from_directory(\n","    'sample_data',\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='binary',\n","    subset='training'\n",")\n","\n","validation_generator = train_datagen.flow_from_directory(\n","    'sample_data',\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='binary',\n","    subset='validation'\n",")\n","\n","def decay_epsilon(epsilon, episode, switch_episode=100):\n","    if episode < switch_episode:\n","        return max(epsilon - 0.01, 0.01)  # Linear decay\n","    else:\n","        return max(epsilon * 0.99, 0.01)  # Exponential decay\n","\n","# Inside the training loop\n","epsilon = decay_epsilon(epsilon, e)  # Adjust epsilon based on the current episode\n"]},{"cell_type":"markdown","id":"9fcd8354-c9ca-49b7-9952-e7973fe6b93d","metadata":{},"outputs":[],"source":["### Summary\n","These exercises are concise and focus on key modifications to the original DQN setup. The code snippets provided are short and simple, encouraging students to think critically about the modifications and how they impact the agent's performance.\n"]},{"cell_type":"markdown","id":"c7815855-a135-41f4-91da-0bda13e09a99","metadata":{},"outputs":[],"source":["### Conclusion \n","\n","Congratulations! You have successfully implemented a Deep Q-Network using Keras to solve the CartPole-v1 environment. You defined a neural network to approximate Q-values, implemented a replay buffer, trained the network using experiences stored in memory, and evaluated the performance of the trained agent. This hands-on exercise reinforced your understanding of DQNs and their implementation in Keras.\n"]},{"cell_type":"markdown","id":"3d801ad7-5ce1-4d3e-b364-025947306607","metadata":{},"outputs":[],"source":["## Authors\n"]},{"cell_type":"markdown","id":"544e1426-1b1d-48ca-8ba6-d44f5d9f3147","metadata":{},"outputs":[],"source":["Skills Network\n"]},{"cell_type":"markdown","id":"e772222c-a89a-4175-a8e9-85c77dd3dc62","metadata":{},"outputs":[],"source":["Copyright © IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"prev_pub_hash":"8f9a22d394782890b42687282f9bc5e03221d5b51569b44b9e5a5dab4e2ce49a"},"nbformat":4,"nbformat_minor":4}